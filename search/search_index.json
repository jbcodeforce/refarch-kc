{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Container Shipment EDA reference implementation The IBM Event Driven architecture reference implementation illustrates the deployment of real time analytics on event streams in the context of container shipment in an event driven architecture with event backbone, functions as service and microservices, and aims to illustrate the different event driven patterns like event sourcing, CQRS and Saga. What you will learn How to apply the event storming methodology and workshop to analyze the business process for fresh good shipment over sees. How to transform Domain Driven Design aggregates to microservices How to implement the different microservices using the event-driven pattern like CQRS pattern with event sourcing done in Apache Kafka or IBM Events Streams How to implement a Test Driven Development for the Order microservice uisng mockito to avoid Kafka dependency Target Audiences You will be greatly interested by the subjects addressed in this solution if you are... An architect, you will get a deeper understanding on how all the components work together, and how to address resiliency, high availability. A developer, you will get a broader view of the solution end to end and get existing starting code, and practices you may want to reuse during your future implementation. We focus on event driven solution in hybrid cloud addressing patterns and non-functional requirements as CI/CD, Test Driven Development, ... A project manager, you may understand all the artifacts to develop in an EDA solution, and we may help in the future to do project estimation. Business process statement In this first chapter we are presenting the business process for shipping fresh good over sees and detailing the event storming analysis workshop execution, and we are explaining how to transform analysis outcomes such as domain boundaries and aggregates to microservices. Design considerations In the third chapter we are going in detail on the process analysis to define the requirements for the major microservices to implement. Architecture This quick architecture note presents the components working together with the event backbone. Deployment To make the solution running we need to have to prepare a set of products installed and running: Event Streams Streaming Analytics * Kubernetes Cluster (IBM Cloud Private or IBM Kubernetes Service on cloud) or Docker compose. We can deploy the components of the solution into three environments: Public cloud (IBM Cloud) , see this article for details on how to prepare the needed services. Private cloud (we are using IBM Cloud Private) and see this article for details. Local to your laptop, using docker images and docker compose.","title":"Home"},{"location":"#container-shipment-eda-reference-implementation","text":"The IBM Event Driven architecture reference implementation illustrates the deployment of real time analytics on event streams in the context of container shipment in an event driven architecture with event backbone, functions as service and microservices, and aims to illustrate the different event driven patterns like event sourcing, CQRS and Saga.","title":"Container Shipment EDA reference implementation"},{"location":"#what-you-will-learn","text":"How to apply the event storming methodology and workshop to analyze the business process for fresh good shipment over sees. How to transform Domain Driven Design aggregates to microservices How to implement the different microservices using the event-driven pattern like CQRS pattern with event sourcing done in Apache Kafka or IBM Events Streams How to implement a Test Driven Development for the Order microservice uisng mockito to avoid Kafka dependency","title":"What you will learn"},{"location":"#target-audiences","text":"You will be greatly interested by the subjects addressed in this solution if you are... An architect, you will get a deeper understanding on how all the components work together, and how to address resiliency, high availability. A developer, you will get a broader view of the solution end to end and get existing starting code, and practices you may want to reuse during your future implementation. We focus on event driven solution in hybrid cloud addressing patterns and non-functional requirements as CI/CD, Test Driven Development, ... A project manager, you may understand all the artifacts to develop in an EDA solution, and we may help in the future to do project estimation.","title":"Target Audiences"},{"location":"#business-process-statement","text":"In this first chapter we are presenting the business process for shipping fresh good over sees and detailing the event storming analysis workshop execution, and we are explaining how to transform analysis outcomes such as domain boundaries and aggregates to microservices.","title":"Business process statement"},{"location":"#design-considerations","text":"In the third chapter we are going in detail on the process analysis to define the requirements for the major microservices to implement.","title":"Design considerations"},{"location":"#architecture","text":"This quick architecture note presents the components working together with the event backbone.","title":"Architecture"},{"location":"#deployment","text":"To make the solution running we need to have to prepare a set of products installed and running: Event Streams Streaming Analytics * Kubernetes Cluster (IBM Cloud Private or IBM Kubernetes Service on cloud) or Docker compose. We can deploy the components of the solution into three environments: Public cloud (IBM Cloud) , see this article for details on how to prepare the needed services. Private cloud (we are using IBM Cloud Private) and see this article for details. Local to your laptop, using docker images and docker compose.","title":"Deployment"},{"location":"introduction/","text":"Introduction As part of producing the IBM event driven point of view and reference architecture, we wanted to bring together a complete scenario which would cover all aspects of developing an event driven solutions including extended connections to devices/IOT and blockchain for trusted business trading networks. We felt that the shipping business could provide a good foundation for this and would enable us to show how to develop event driven solutions following the architecture patterns. Business process statement The high level process can be represented in the following diagram, and is described in detailed in this section : In developing the scenario, it became apparent that the event driven nature of business, extends across the business network, so we have widened the view in the scenario to consider the chain of parties involved in the shipping process, including importer, exporter, land transport and customs. To keep the scenario easy to understand, we have only considered the following cases: Importer Orders goods from exporter overseas Exporter becomes the customer of the shipping agent and uses 'K.Container' shipping service Shipping agent manages process of land transport loading, unloading and shipping. Through the scenario we can see the impact of \u201cevents\u201d, which may delay or change the shipping process across all three parties. We are using goods to be transported in refrigerator containers or reefer containers to keep the 'cold chain' of transported products. Event storming analysis We met with the business users and project stakeholder during an event storming workshop, and we are detailing the outcomes in the next chapter >>","title":"Introduction"},{"location":"introduction/#introduction","text":"As part of producing the IBM event driven point of view and reference architecture, we wanted to bring together a complete scenario which would cover all aspects of developing an event driven solutions including extended connections to devices/IOT and blockchain for trusted business trading networks. We felt that the shipping business could provide a good foundation for this and would enable us to show how to develop event driven solutions following the architecture patterns.","title":"Introduction"},{"location":"introduction/#business-process-statement","text":"The high level process can be represented in the following diagram, and is described in detailed in this section : In developing the scenario, it became apparent that the event driven nature of business, extends across the business network, so we have widened the view in the scenario to consider the chain of parties involved in the shipping process, including importer, exporter, land transport and customs. To keep the scenario easy to understand, we have only considered the following cases: Importer Orders goods from exporter overseas Exporter becomes the customer of the shipping agent and uses 'K.Container' shipping service Shipping agent manages process of land transport loading, unloading and shipping. Through the scenario we can see the impact of \u201cevents\u201d, which may delay or change the shipping process across all three parties. We are using goods to be transported in refrigerator containers or reefer containers to keep the 'cold chain' of transported products.","title":"Business process statement"},{"location":"introduction/#event-storming-analysis","text":"We met with the business users and project stakeholder during an event storming workshop, and we are detailing the outcomes in the next chapter >>","title":"Event storming analysis"},{"location":"analysis/readme/","text":"Container Shipment Analysis This section defines the overall steps in the methodology to analyse a specific global shipping example and derive the event driven solution. We combined some elements of the design thinking methodology with the event storming and domain driven design to extract the following analysis of the business domain. Output from Domain Driven Design workshop From the design thinking workshop we extracted the following artifacts: a persona list the MVP hills Personas for each stakeholder We develop personas for each of the business stakeholders to better understand their work environment, motivations and challenges. Personas helps to capture the decisions and questions that these stakeholders must address with respect to the targeted key business initiative. Persona name Objectives Challenges Retailer Receive shipped goods on time, on date contracted with manufacturer Receive assurance that temperature sensitive goods have remained with bounds Late delivery may miss market opportunity long delivery time makes market opportunitiy prediction more difficult Manufacturer Good enough estimates of shipment times from Shipment Company to close sale and delivery with Retailer Pickup of containers by land transporter Timely delivery of container to Retailer as contracted with Shipment company Able to get information on current location and state of container in transit Contract with Shipment company will include timing estimates and penalty clauses must update Retailer as sonn as schedule changes known Must receive and communicate to retailer assurance on history of temperature sensitive goods Shipping Company Provide good enough estimates of shipment time to close shipment contract with Manufacturer Execute shipment contracts on time profitably ( with minimal cost) Fixed ship and itinerary schedule variability in ship leg travel times and costs variability in port congestion and load / unload times at dock variability in Land transport timings Land Transporter Pick up and drop off containers at times and locations agreed with Shipment company May be short notice requests may actually use bids in a market to resolve at lowest cost best response etc. Port Dock Operator Load and unload containers from docked ship as specified by Shipping Company with minimal time and effort free up dock asset quickly to become available for next ship Highly complex sequence of operation in Dockyard to be coordinated to minimize effort and time Customs Officer Clear containers for export and assess duty on import containers Depends on quality of manifest and certification of origin documentation for each container from Manufacturer MVP hills The challenges listed in the persona table above identify a possible set of MVP hills for this end to end solution. The event storming methodology described below will lead to picking out specific subareas of the solution with most value as initial MVPs. High level view of the shipment process flow At the high level the shipment process flow is suggested and illustrated in the diagram below. For the purposes of showing how to design a reference EDA solution we select on a simple subcase of all actual and possible variations of the global container flow. Very high level steps in this flow are as follows: Retailer and manufacturer interact to create agreement to deliver specific goods in a container from manufacturer's location to retailer's location with an expected arrival date. Manufacturer places shipping order with 'Shipping Company' to pickup container and deliver under the condition expected above. Shipping Company arranges for land transport to pick up loaded container and required documentation from Manufacturer and deliver the container to dockside at source port (adjacent to Maufacturer) for loading onto container carrier. Shipping company works with Customs Officer at source port to clear outbound container for export. When Container Ship is in dock at source port Shipping company arranges with Port Dock Operator to load and unload containers at this port. Loaded container ship leaves dock in source port adjacent to Manufacturer and sails to destination port. Container ship arrives at destination port (adjacent to Retailer) and queues to enter Port Docking area. Shipment company arranges with Port Docking Operator to unload specific containers needed at this port and reload additional ones for next shipping leg. Shipment company works with Import Export office at destination port to clear and collect any import duties. Shipment company works with Land Transporter at destination port to pick up container and deliver to Retailer. Container is delivered by Land Transporter to Retailer's location - transaction is complete. Event storming analysis of the container shipping flow We use the event storming analysis to move from the high level description of a business process flow above to a specific event timeline with identified bounded contexts each of which could be a target MVP component linked through EDA architecture. Event storming is a rapid lightweight design process enabling the team of business owners and stake holders, architects and IT specialists to fomalize a complex solution in a clearly communicable event timeline. This step is effective in developing event-based microservices linked through an EDA architecture in one or more MVP contexts. Steps in an eight hours event storming analysis workshop of the container shipping example are illustrated and described below. Step 1: Capture the Domain Event Timeline, swim lanes and key phases (This section of the example description covers activities identified as event storming workshop steps 1,2,3 in the generic description of the event storming method . The initial step in event storming analysis is to capture all events, things which have happened at a point in time, and organize them into a timeline: each event goes on an orange \"sticky note\" parallel or independent processes may be separated with blue horizontal swim lanes critical event indicate a new stage, or pivot, in the flow shown with vertical blue bars. For the global shipment example described at a very high level above we came up with an event timeline shown in the set of diagrams below. (The event storming process captures these event timeline sections in charts on walls around the meeting room). Container shipping event timeline section 1 This section of the event time line deals with initial contracts to ship container and startup actions - specifically: Retailer and Manufacturer settling on an initial order for delivery of goods in a container. Manufacturer placing order for shipment with Shipping Company. Land transport arranged to pick up container and deliver to source port. Container ship approach source port adjacent to Manufacturer's location. The events are organized into separate swim lanes for Manufacturer, Retailer and Ship perspectives operating in parallel. Swim lanes help to clearly separate ship events as it approaches the source port from container specific events with agreements to ship etc. There is no time coupling or precise causality between events in these two swim lanes. The red sticky note is a comment. In this case we make the particular simplification to limit the scenario to shipping complete containers only. This avoids having to deal with additional warehousing, container load aggregation and packing events - together with correspondng unpacking and disaggregation. Container shipping event timeline section 2 This section continues event time line development with a swim lane now focussing on loading and pickup of a specific container at the Manufacturer's location and its delivery to the source port dockside. There is a critical event (indicated by vertical blue bar) separating the \"source dockside\" phase of the solution. Before this critical event we are dealing with container specific activities in collecting and transporting the container from Manufacturer's location to dockside. In the following dockside phase there are interactions with Customs Officer to get the container cleared for export. The Manufacturer will need an empty container (refrigerated if necessary for the shipment of interest) to load the goods into. We show an event for empty container being delivered. The solution is simplified if we assume that the Manufacture has a pool of empty containers always available. Alternatively this can be analyzed fully in some more complete generalized version of the solution. When the container arrives at source port dockside it may or may not be intime for the cutoff time required by the Customs Officer to get containers cleared for export before the scheduled departure of a particular container ship. If the cutoff deadline is missed the shipment will need to be rebooked on a later container ship and the client Manufacturer notified of expected delay in delivery. Container shipping event timeline section 3 This section continues the event timelines with swim lanes relating to a specific container shipment and also to the movement of a ship potentially carrying hundreds of containers. It introduces two new critical events: The Customs decision phase of event ends with a specific decision to clear a container for export or not, or possibly a request for additional inspecions or documents requiring more decision time * If the container is approved for export it can proceed to loading. * If additional time is required for the clearance process, the original booking and expected delivery date may need to be modified. * If export clearance is denied, then shipmen is cancelled and requesting parties notified. Ship enters dock ready to start unloading and loading is a new critical event. * Previous ship events in Event Timeline section 1 dealt with ship \"booking\" a load/unload timeslot at a dock in the source port * Also getting national authority or Customs clearance to enter that jurisdiction * Now on arrival at the source port anchorage area, the ship requests permission to moor at an available dock facility * The critical event when a ship is cleared and moored at a dock hence ready to start unloading and loading containers is the start of the next event phase - container loading (and unloading) Container shipping event timeline section 4 This segment of the event timeline deals with a single swim lane for the ship while it is moored in a dock facility at the source port, is having arriving containers destined for this port unloaded and new containers being loaded at his port. The port dock facility operator is coordinating many events in the yard to perform load unload operations. These steps - as noted in a red discussion \"sticky\" in the event storming timeline are repeated for many containers. The time line presented here captures representative high level events. It is straightforward to extend the analysis to open up additional layers of detail touching on operational optimizations and coordination at the cost of addiional complexity not essential to our reference example here. Some of the events in this phase are now specialized to address needs of particular type of container - refrigerated containers - able to maintain specific temperature bounds and to report on their global location and temperature status on a continuous basis. This is a natural outcome of the event storming analysis involving free parallel capture of event types by a team of individuals with different points of view and interests. Working forward towards one or more MVP implementations of key components of this solution linked through EDA architecture we will need to characterize event types more uniformly end to end but imposing that level of consistency checking on the initial event storming process will slow down progess without providing significant benefit. Container shipping event timeline section 5 This segment of the event timeline captures events which occure in the blue water phase of the shipping, after the container ship has left the source port and is travelling owards but has not yet reached the destination port. It is divided into two swim lanes the ship perspective and individual container perspectives. The ship perspective includes events relating to the entire ship: leaving port. reporting its current position. deciding to alter planned course to avoid a weather event. The upper swim lane capture events which are specific to a particular container. container sensors reporting on geolocation. refrigerated container sensors reporting on humidity, carbon dioxide, temperature in the container and power consumption of the refrigeration unit. Container shipping event timeline sections 6 and 7 The remining event time line segments 6 and 7 deal with arrival at the destination port unload of the container and delivery to the Retailer's location. At the level of simplification in the reference architecture example, the steps for unloading a container at the destination port, clearing Customs and delivering it to are Retailer location are the symmetric image of the steps to pick up the container from the Manufacture location, clear it through export permissions and load onto the ship. For these reason we just provide event timeline digrams for these steps withou going into further explanatory detail. Step 2: Identify commands and event linkages This section of the example description covers activities identified as event storming workshop steps 4,5) in the generic description of the event storming method . After capturing all events for the scenario and organizing them in a time line, the next step in event storming analysis is to identify the triggers for events and causal linkages between events. For each identified event in the timeline we ask \"What triggered this event to occur?\". Expected event trigger types are: A human operator makes a decision and issues a command. Some external system or sensor provides a stimulus. An event results from some policy - typically automated processing of a precursor event. Completion of some determined period of elapsed time. For each event trigerred by a command the triggering command is identified in a blue (sticky) note * this may become a microservice api in a later implementation the human persona issuing the command is identified and shown in a yellow note above this. For events trigerred by processing of some precursor events the trigerring policy explaining when and why this event occurs is summarized in a lilac colored note. Specific causal event linkages are added to the event storming diagram as blue directed (arrow) linkages. In the following subsections we show the results of command and event linkage analysis for some selected areas of the container shipping example. Container shipping Commands for order placement and land transport setup This diagram shows the command, agent issuing events and policies triggering events for the order placement and land transport set up (at manufacturer location) sections of the event timeline generated in step 1 Container shipping event linkages for order placement setup The above diagram adds event linkages showing the causality chaining of events and business rules. Container shipping commands for pickup at Manufacturer's location The above diagram is generated for the command and policies associated with pick up of a loaded container from the Manufacturer's location and delivery to the source port dockside. Container shipping commands in port to port (Blue water) section of the event time line The diagram is self explanatory. Step 3: Decision data, predictive insights and insight storming: This section of the example description covers activities identified as event storming workshop step 8 in the generic description of the event storming method . Insight storming is extending the event storming workshop to identify and capture insightful predictive analytics, and it is introduced and described in workshop execution Step 8 - Insight . Predictive analytic insights are effectively probabilistic statements about which future events are likely to occur and what are the like properties of those events. They are typicaly generated using models created by data scientists or using artificial intelligence (AI) or machine learning (ML). Business owners and stakeholders in the event driven solution have good intuitions on: Which probabilistic insights are likely to lead to better decision making and action when a particular event occurs. What sources of information are likely to help create a model to predict this insight. So in event storming for an EDA system, we recommend generalizing the step of identifying data (properties of past definite events) to help make good decision and replacing this with an insight storming step which will look for: data which will help make good decisions about how to act when an event occurs predictive insights which could help guide our actions in response to proactively before some future event. * sources of data which are likely to enable the building of reliable predictive insight models. This additional step of insight storming takes advantage of the fact that we already have a time line for the problem being analysed with all events designed, commands, policies and event linkages already identified, and the business owners and stakeholders in the room whose insights for the business problem enable them to identify potentially valuable predictive insights. Working through insight storming in this way leads to a business value driven specification of possible predictive analytics opportunities in the solution. Event driven architecture provides a mature pattern to models addressing the identified needs. Event Stream Processing analytics infrastructure is then availalable to support scoring of these models and uses of the resulting insights in decision making and action in real time. Container shipping event stream processing diagram - including event stream processing flows The shipping example includes the case where continuous sensor measurement data is available from each refrigerated container while it is stacked on board the container ship and is in between ports on a blue water phase of the scenario. We show how streaming analytics can process the arriving continuous sensor measures in real-time and to deliver additional capabilites in the EDA solution. A diagram for this flow generated from Insight storming is shown below. In this diagram it is made clear the delivery of measured temperature, probably GPS position, and power consumption of the refrigeration unit for that container is a recurring \"continuous\" event stream. Each container might report once a minute; this ensures that an auditable record of container temperature is available from the event backbone or event sourcing. We show a policy test to decide whether the temperature has gone outside the specified range committed to in that shipment contract for the goods in that container. If this violation has occured, this is an (unusual) alert event reporting that temperature has gone out of range. This information is available as data to subject matter expert's dashboard seen by the shipping company operator who must make the business decision whether the contents of the container are spoiled. It is likely that involvement of human operator is necessary since this is a business decision with possibly significant $ consequences. It is possible that a bad sensor reading could have been received or that in this contract violation of the temperature range for a very short interval of time is permissable. Some stateful analysis of the container temperature reports would make the readings more reliable; perhaps there need to be more than one out of range reading to issue the alert to avoid corrupted data false positives. If the business decision is made that the container's contents are spoiled: A command is invoked to act on this decision. The container refrigeration may be powered down (possible other sensing left active) A policy based on terms and class of service of this particular shipment will determine: Whether a replacement shipment will be initiated and booked Usually shipping and receiving parties need to be notified The shipping company will schedule some salvage or disposal action for the content of the container at next port Each of the actions above will be an event captured in the event backbone - trigerring further loosely coupled commands and policies to take defined actions. Container shipping event stream processing with predictive Insight flows included The previous section defines how event stream processing could detect when a shipment was spoiled and trigger recovery actions. But shipping experts in an insight storming session will note that it is much better to predict when a spoilage temperature event is likely to occur and to take automated immediate (real-time) action to avert the spoilage. The simplest form of prediction of a temperature likely to go outside of its permissible range is to have checks on whether the temperature is approaching these bounds. If the temperature must stay below T degrees, take corrective action if it reaches T - delta degrees. More complex models for predicting temperature, could take into account diurnal variation due to external temperatures, possible predicted external temperatures forecast for the current course of the ship, and whether ther container is stacked above deck and hence particularly exposed to external temperatures. We assume that possible corrective action includes resetting the thermostatic controls on the refrigeration unit for the cotainer, possibly resetting the controls which may have drifted from their calibrated settings... An insight storming diagram which could be generated from discussion of these potentially useful insights and predictions is shown in the diagram below. We have added an additional insight - namely that it may be possible to predict from the temperature observed in a container and the trend of power consumption of that refrigeration unit, that the unit is in danger of failing and should be inspected and possibly services as soon as possible. Insights about predicted risk of temperature based spoilage, and prediction of refrigeration unit probable need for maintenance are presented in light blue. These are probabilistic prediction for properties and likely occurence of future events. Loose coupling and reuse of these insights by allowing publish subscribe to insight topics is helpful. Insights are conceptually different from events since they are probabilistic predictions for the future rather than events which by definition have already happened at some specific point in time. Event stream processing for insights relating to the ship Step 4: Commands, linkages, data and context for order placement This section of the example description covers activities identified as EventStorming Workshop steps 6,7 in the generic description of the event storming method . In particular, we look at identifying bounded contexts and identifying aggregates which will lead to a loosely coupled collection of microservices providing an agile EDA design for the example. We drill down on understanding the order placement process when a container shipment is booked as the MVP context focus in which to explore our design at the next level of detail.","title":"Event storming analysis"},{"location":"analysis/readme/#container-shipment-analysis","text":"This section defines the overall steps in the methodology to analyse a specific global shipping example and derive the event driven solution. We combined some elements of the design thinking methodology with the event storming and domain driven design to extract the following analysis of the business domain.","title":"Container Shipment Analysis"},{"location":"analysis/readme/#output-from-domain-driven-design-workshop","text":"From the design thinking workshop we extracted the following artifacts: a persona list the MVP hills","title":"Output from Domain Driven Design workshop"},{"location":"analysis/readme/#personas-for-each-stakeholder","text":"We develop personas for each of the business stakeholders to better understand their work environment, motivations and challenges. Personas helps to capture the decisions and questions that these stakeholders must address with respect to the targeted key business initiative. Persona name Objectives Challenges Retailer Receive shipped goods on time, on date contracted with manufacturer Receive assurance that temperature sensitive goods have remained with bounds Late delivery may miss market opportunity long delivery time makes market opportunitiy prediction more difficult Manufacturer Good enough estimates of shipment times from Shipment Company to close sale and delivery with Retailer Pickup of containers by land transporter Timely delivery of container to Retailer as contracted with Shipment company Able to get information on current location and state of container in transit Contract with Shipment company will include timing estimates and penalty clauses must update Retailer as sonn as schedule changes known Must receive and communicate to retailer assurance on history of temperature sensitive goods Shipping Company Provide good enough estimates of shipment time to close shipment contract with Manufacturer Execute shipment contracts on time profitably ( with minimal cost) Fixed ship and itinerary schedule variability in ship leg travel times and costs variability in port congestion and load / unload times at dock variability in Land transport timings Land Transporter Pick up and drop off containers at times and locations agreed with Shipment company May be short notice requests may actually use bids in a market to resolve at lowest cost best response etc. Port Dock Operator Load and unload containers from docked ship as specified by Shipping Company with minimal time and effort free up dock asset quickly to become available for next ship Highly complex sequence of operation in Dockyard to be coordinated to minimize effort and time Customs Officer Clear containers for export and assess duty on import containers Depends on quality of manifest and certification of origin documentation for each container from Manufacturer","title":"Personas for each stakeholder"},{"location":"analysis/readme/#mvp-hills","text":"The challenges listed in the persona table above identify a possible set of MVP hills for this end to end solution. The event storming methodology described below will lead to picking out specific subareas of the solution with most value as initial MVPs.","title":"MVP hills"},{"location":"analysis/readme/#high-level-view-of-the-shipment-process-flow","text":"At the high level the shipment process flow is suggested and illustrated in the diagram below. For the purposes of showing how to design a reference EDA solution we select on a simple subcase of all actual and possible variations of the global container flow. Very high level steps in this flow are as follows: Retailer and manufacturer interact to create agreement to deliver specific goods in a container from manufacturer's location to retailer's location with an expected arrival date. Manufacturer places shipping order with 'Shipping Company' to pickup container and deliver under the condition expected above. Shipping Company arranges for land transport to pick up loaded container and required documentation from Manufacturer and deliver the container to dockside at source port (adjacent to Maufacturer) for loading onto container carrier. Shipping company works with Customs Officer at source port to clear outbound container for export. When Container Ship is in dock at source port Shipping company arranges with Port Dock Operator to load and unload containers at this port. Loaded container ship leaves dock in source port adjacent to Manufacturer and sails to destination port. Container ship arrives at destination port (adjacent to Retailer) and queues to enter Port Docking area. Shipment company arranges with Port Docking Operator to unload specific containers needed at this port and reload additional ones for next shipping leg. Shipment company works with Import Export office at destination port to clear and collect any import duties. Shipment company works with Land Transporter at destination port to pick up container and deliver to Retailer. Container is delivered by Land Transporter to Retailer's location - transaction is complete.","title":"High level view of the shipment process flow"},{"location":"analysis/readme/#event-storming-analysis-of-the-container-shipping-flow","text":"We use the event storming analysis to move from the high level description of a business process flow above to a specific event timeline with identified bounded contexts each of which could be a target MVP component linked through EDA architecture. Event storming is a rapid lightweight design process enabling the team of business owners and stake holders, architects and IT specialists to fomalize a complex solution in a clearly communicable event timeline. This step is effective in developing event-based microservices linked through an EDA architecture in one or more MVP contexts. Steps in an eight hours event storming analysis workshop of the container shipping example are illustrated and described below.","title":"Event storming analysis of the container shipping flow"},{"location":"analysis/readme/#step-1-capture-the-domain-event-timeline-swim-lanes-and-key-phases","text":"(This section of the example description covers activities identified as event storming workshop steps 1,2,3 in the generic description of the event storming method . The initial step in event storming analysis is to capture all events, things which have happened at a point in time, and organize them into a timeline: each event goes on an orange \"sticky note\" parallel or independent processes may be separated with blue horizontal swim lanes critical event indicate a new stage, or pivot, in the flow shown with vertical blue bars. For the global shipment example described at a very high level above we came up with an event timeline shown in the set of diagrams below. (The event storming process captures these event timeline sections in charts on walls around the meeting room).","title":"Step 1: Capture the Domain Event Timeline, swim lanes and key phases"},{"location":"analysis/readme/#container-shipping-event-timeline-section-1","text":"This section of the event time line deals with initial contracts to ship container and startup actions - specifically: Retailer and Manufacturer settling on an initial order for delivery of goods in a container. Manufacturer placing order for shipment with Shipping Company. Land transport arranged to pick up container and deliver to source port. Container ship approach source port adjacent to Manufacturer's location. The events are organized into separate swim lanes for Manufacturer, Retailer and Ship perspectives operating in parallel. Swim lanes help to clearly separate ship events as it approaches the source port from container specific events with agreements to ship etc. There is no time coupling or precise causality between events in these two swim lanes. The red sticky note is a comment. In this case we make the particular simplification to limit the scenario to shipping complete containers only. This avoids having to deal with additional warehousing, container load aggregation and packing events - together with correspondng unpacking and disaggregation.","title":"Container shipping event timeline section 1"},{"location":"analysis/readme/#container-shipping-event-timeline-section-2","text":"This section continues event time line development with a swim lane now focussing on loading and pickup of a specific container at the Manufacturer's location and its delivery to the source port dockside. There is a critical event (indicated by vertical blue bar) separating the \"source dockside\" phase of the solution. Before this critical event we are dealing with container specific activities in collecting and transporting the container from Manufacturer's location to dockside. In the following dockside phase there are interactions with Customs Officer to get the container cleared for export. The Manufacturer will need an empty container (refrigerated if necessary for the shipment of interest) to load the goods into. We show an event for empty container being delivered. The solution is simplified if we assume that the Manufacture has a pool of empty containers always available. Alternatively this can be analyzed fully in some more complete generalized version of the solution. When the container arrives at source port dockside it may or may not be intime for the cutoff time required by the Customs Officer to get containers cleared for export before the scheduled departure of a particular container ship. If the cutoff deadline is missed the shipment will need to be rebooked on a later container ship and the client Manufacturer notified of expected delay in delivery.","title":"Container shipping event timeline section 2"},{"location":"analysis/readme/#container-shipping-event-timeline-section-3","text":"This section continues the event timelines with swim lanes relating to a specific container shipment and also to the movement of a ship potentially carrying hundreds of containers. It introduces two new critical events: The Customs decision phase of event ends with a specific decision to clear a container for export or not, or possibly a request for additional inspecions or documents requiring more decision time * If the container is approved for export it can proceed to loading. * If additional time is required for the clearance process, the original booking and expected delivery date may need to be modified. * If export clearance is denied, then shipmen is cancelled and requesting parties notified. Ship enters dock ready to start unloading and loading is a new critical event. * Previous ship events in Event Timeline section 1 dealt with ship \"booking\" a load/unload timeslot at a dock in the source port * Also getting national authority or Customs clearance to enter that jurisdiction * Now on arrival at the source port anchorage area, the ship requests permission to moor at an available dock facility * The critical event when a ship is cleared and moored at a dock hence ready to start unloading and loading containers is the start of the next event phase - container loading (and unloading)","title":"Container shipping event timeline section 3"},{"location":"analysis/readme/#container-shipping-event-timeline-section-4","text":"This segment of the event timeline deals with a single swim lane for the ship while it is moored in a dock facility at the source port, is having arriving containers destined for this port unloaded and new containers being loaded at his port. The port dock facility operator is coordinating many events in the yard to perform load unload operations. These steps - as noted in a red discussion \"sticky\" in the event storming timeline are repeated for many containers. The time line presented here captures representative high level events. It is straightforward to extend the analysis to open up additional layers of detail touching on operational optimizations and coordination at the cost of addiional complexity not essential to our reference example here. Some of the events in this phase are now specialized to address needs of particular type of container - refrigerated containers - able to maintain specific temperature bounds and to report on their global location and temperature status on a continuous basis. This is a natural outcome of the event storming analysis involving free parallel capture of event types by a team of individuals with different points of view and interests. Working forward towards one or more MVP implementations of key components of this solution linked through EDA architecture we will need to characterize event types more uniformly end to end but imposing that level of consistency checking on the initial event storming process will slow down progess without providing significant benefit.","title":"Container shipping event timeline section 4"},{"location":"analysis/readme/#container-shipping-event-timeline-section-5","text":"This segment of the event timeline captures events which occure in the blue water phase of the shipping, after the container ship has left the source port and is travelling owards but has not yet reached the destination port. It is divided into two swim lanes the ship perspective and individual container perspectives. The ship perspective includes events relating to the entire ship: leaving port. reporting its current position. deciding to alter planned course to avoid a weather event. The upper swim lane capture events which are specific to a particular container. container sensors reporting on geolocation. refrigerated container sensors reporting on humidity, carbon dioxide, temperature in the container and power consumption of the refrigeration unit.","title":"Container shipping event timeline section 5"},{"location":"analysis/readme/#container-shipping-event-timeline-sections-6-and-7","text":"The remining event time line segments 6 and 7 deal with arrival at the destination port unload of the container and delivery to the Retailer's location. At the level of simplification in the reference architecture example, the steps for unloading a container at the destination port, clearing Customs and delivering it to are Retailer location are the symmetric image of the steps to pick up the container from the Manufacture location, clear it through export permissions and load onto the ship. For these reason we just provide event timeline digrams for these steps withou going into further explanatory detail.","title":"Container shipping event timeline sections 6 and 7"},{"location":"analysis/readme/#step-2-identify-commands-and-event-linkages","text":"This section of the example description covers activities identified as event storming workshop steps 4,5) in the generic description of the event storming method . After capturing all events for the scenario and organizing them in a time line, the next step in event storming analysis is to identify the triggers for events and causal linkages between events. For each identified event in the timeline we ask \"What triggered this event to occur?\". Expected event trigger types are: A human operator makes a decision and issues a command. Some external system or sensor provides a stimulus. An event results from some policy - typically automated processing of a precursor event. Completion of some determined period of elapsed time. For each event trigerred by a command the triggering command is identified in a blue (sticky) note * this may become a microservice api in a later implementation the human persona issuing the command is identified and shown in a yellow note above this. For events trigerred by processing of some precursor events the trigerring policy explaining when and why this event occurs is summarized in a lilac colored note. Specific causal event linkages are added to the event storming diagram as blue directed (arrow) linkages. In the following subsections we show the results of command and event linkage analysis for some selected areas of the container shipping example.","title":"Step 2: Identify commands and event linkages"},{"location":"analysis/readme/#container-shipping-commands-for-order-placement-and-land-transport-setup","text":"This diagram shows the command, agent issuing events and policies triggering events for the order placement and land transport set up (at manufacturer location) sections of the event timeline generated in step 1","title":"Container shipping Commands for order placement and land transport setup"},{"location":"analysis/readme/#container-shipping-event-linkages-for-order-placement-setup","text":"The above diagram adds event linkages showing the causality chaining of events and business rules.","title":"Container shipping  event linkages for order placement setup"},{"location":"analysis/readme/#container-shipping-commands-for-pickup-at-manufacturers-location","text":"The above diagram is generated for the command and policies associated with pick up of a loaded container from the Manufacturer's location and delivery to the source port dockside.","title":"Container shipping commands for pickup at Manufacturer's location"},{"location":"analysis/readme/#container-shipping-commands-in-port-to-port-blue-water-section-of-the-event-time-line","text":"The diagram is self explanatory.","title":"Container shipping commands in port to port (Blue water) section of the event time line"},{"location":"analysis/readme/#step-3-decision-data-predictive-insights-and-insight-storming","text":"This section of the example description covers activities identified as event storming workshop step 8 in the generic description of the event storming method . Insight storming is extending the event storming workshop to identify and capture insightful predictive analytics, and it is introduced and described in workshop execution Step 8 - Insight . Predictive analytic insights are effectively probabilistic statements about which future events are likely to occur and what are the like properties of those events. They are typicaly generated using models created by data scientists or using artificial intelligence (AI) or machine learning (ML). Business owners and stakeholders in the event driven solution have good intuitions on: Which probabilistic insights are likely to lead to better decision making and action when a particular event occurs. What sources of information are likely to help create a model to predict this insight. So in event storming for an EDA system, we recommend generalizing the step of identifying data (properties of past definite events) to help make good decision and replacing this with an insight storming step which will look for: data which will help make good decisions about how to act when an event occurs predictive insights which could help guide our actions in response to proactively before some future event. * sources of data which are likely to enable the building of reliable predictive insight models. This additional step of insight storming takes advantage of the fact that we already have a time line for the problem being analysed with all events designed, commands, policies and event linkages already identified, and the business owners and stakeholders in the room whose insights for the business problem enable them to identify potentially valuable predictive insights. Working through insight storming in this way leads to a business value driven specification of possible predictive analytics opportunities in the solution. Event driven architecture provides a mature pattern to models addressing the identified needs. Event Stream Processing analytics infrastructure is then availalable to support scoring of these models and uses of the resulting insights in decision making and action in real time.","title":"Step 3: Decision data, predictive insights and insight storming:"},{"location":"analysis/readme/#container-shipping-event-stream-processing-diagram-including-event-stream-processing-flows","text":"The shipping example includes the case where continuous sensor measurement data is available from each refrigerated container while it is stacked on board the container ship and is in between ports on a blue water phase of the scenario. We show how streaming analytics can process the arriving continuous sensor measures in real-time and to deliver additional capabilites in the EDA solution. A diagram for this flow generated from Insight storming is shown below. In this diagram it is made clear the delivery of measured temperature, probably GPS position, and power consumption of the refrigeration unit for that container is a recurring \"continuous\" event stream. Each container might report once a minute; this ensures that an auditable record of container temperature is available from the event backbone or event sourcing. We show a policy test to decide whether the temperature has gone outside the specified range committed to in that shipment contract for the goods in that container. If this violation has occured, this is an (unusual) alert event reporting that temperature has gone out of range. This information is available as data to subject matter expert's dashboard seen by the shipping company operator who must make the business decision whether the contents of the container are spoiled. It is likely that involvement of human operator is necessary since this is a business decision with possibly significant $ consequences. It is possible that a bad sensor reading could have been received or that in this contract violation of the temperature range for a very short interval of time is permissable. Some stateful analysis of the container temperature reports would make the readings more reliable; perhaps there need to be more than one out of range reading to issue the alert to avoid corrupted data false positives. If the business decision is made that the container's contents are spoiled: A command is invoked to act on this decision. The container refrigeration may be powered down (possible other sensing left active) A policy based on terms and class of service of this particular shipment will determine: Whether a replacement shipment will be initiated and booked Usually shipping and receiving parties need to be notified The shipping company will schedule some salvage or disposal action for the content of the container at next port Each of the actions above will be an event captured in the event backbone - trigerring further loosely coupled commands and policies to take defined actions.","title":"Container shipping event stream processing diagram - including event stream processing flows"},{"location":"analysis/readme/#container-shipping-event-stream-processing-with-predictive-insight-flows-included","text":"The previous section defines how event stream processing could detect when a shipment was spoiled and trigger recovery actions. But shipping experts in an insight storming session will note that it is much better to predict when a spoilage temperature event is likely to occur and to take automated immediate (real-time) action to avert the spoilage. The simplest form of prediction of a temperature likely to go outside of its permissible range is to have checks on whether the temperature is approaching these bounds. If the temperature must stay below T degrees, take corrective action if it reaches T - delta degrees. More complex models for predicting temperature, could take into account diurnal variation due to external temperatures, possible predicted external temperatures forecast for the current course of the ship, and whether ther container is stacked above deck and hence particularly exposed to external temperatures. We assume that possible corrective action includes resetting the thermostatic controls on the refrigeration unit for the cotainer, possibly resetting the controls which may have drifted from their calibrated settings... An insight storming diagram which could be generated from discussion of these potentially useful insights and predictions is shown in the diagram below. We have added an additional insight - namely that it may be possible to predict from the temperature observed in a container and the trend of power consumption of that refrigeration unit, that the unit is in danger of failing and should be inspected and possibly services as soon as possible. Insights about predicted risk of temperature based spoilage, and prediction of refrigeration unit probable need for maintenance are presented in light blue. These are probabilistic prediction for properties and likely occurence of future events. Loose coupling and reuse of these insights by allowing publish subscribe to insight topics is helpful. Insights are conceptually different from events since they are probabilistic predictions for the future rather than events which by definition have already happened at some specific point in time.","title":"Container shipping event stream processing with predictive Insight flows included"},{"location":"analysis/readme/#event-stream-processing-for-insights-relating-to-the-ship","text":"","title":"Event stream processing for insights relating to the ship"},{"location":"analysis/readme/#step-4-commands-linkages-data-and-context-for-order-placement","text":"This section of the example description covers activities identified as EventStorming Workshop steps 6,7 in the generic description of the event storming method . In particular, we look at identifying bounded contexts and identifying aggregates which will lead to a loosely coupled collection of microservices providing an agile EDA design for the example. We drill down on understanding the order placement process when a container shipment is booked as the MVP context focus in which to explore our design at the next level of detail.","title":"Step 4: Commands, linkages, data and context for order placement"},{"location":"demo/readme/","text":"Demo Script This demo script is using the localhost deployment with a mapping of the host name kcsolution to localhost defined in the /etc/hosts file. For IBM Cloud change the hostname accordingly. Here is how to execute the business process step by step: Step 1: Manufacturer create an order: Go to the http://kcsolution:3110 URL to access the demonstration home page: This page presents the process and some tiles that can be used to simulate the progression within the business process. The grey shadowed tiles are not actives. From the Initiate Orders - Manufacturer we can have the manufacturer creating a new fresh product order to ship over sea. To represent different manufacturer the first select box is used to support different scenarios in the future. GoodManufacturer can be used. Then a list of existing orders may be displayed. You can add order with the UI, but you can also use a script in the order command microservice project: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms/blob/master/order-command-ms/scripts/createOrder.sh Here is an example to add an order to get a voyage from Oakland to Shanghai: ./createOrder.sh localhost:10080 ./orderOacklandToChinaCreateonso There is a lot happening here. The Angular is getting orders using the orders.service.ts service from the BFF at the address: http://localhost:3010/api/orders . The BFF is calling the Order Query Microservice via a javascript client code: getOrders(manuf) function. . The Order Query microservice URL is defined in environment variable or defaulted in the config file. It is mapped to the deployed Order service. (e.g. http://ordercmd:9080/orders) Selecting one order using the Arrow icon open the details of the order: As illustrated in the CQRS diagram: the creation of the order goes to the order command microservice which publishes a OrderCreated event to the orders topic and then consumes it to persist the data to its database. See source code here If you plug a orders consumer you can see the following trace wiht the status of the order being pending and the type of event being OrderCreated {\"payload\":{ \"orderID\":\"1fcccdf2-e29d-4b30-8e52-8116dc2a01ff\", \"productID\":\"Carrot\", \"customerID\":\"GoodManuf\", \"quantity\":10000, \"pickupAddress\": \"...\", \"expectedDeliveryDate\":\"2019-03-31T13:30Z\", \"status\":\"pending\"}, \"type\":\"OrderCreated\", \"version\":\"1\"} Step 2: K Container Shipment Manager looking at Orders From the home page goes to the Shipment Inc tile: Then the home page lists the current order the shipment company received The status of those events will be modified over time while the order is processed down stream by the voyage and container services. The following sequence diagram illustrates the flow: Looking at the traces in the voyage service voyages_1 | emitting {\"timestamp\":1548788544290,\"type\":\"OrderAssigned\",\"version\":\"1\",\"payload\":{\"voyageID\":100,\"orderID\":\"1fcccdf2-e29d-4b30-8e52-8116dc2a01ff\"}} or at the orders topic: {\"timestamp\":1548792921679, \"type\":\"OrderAssigned\",\"version\":\"1\", \"payload\":{\"voyageID\":100,\"orderID\":\"1fcccdf2-e29d-4b30-8e52-8116dc2a01ff\"}} Step3: Simulate the ship in blue water From the home page goes to the Simulate the bluewater tile, then in the main page select one of the available fleet. Only the North Pacific has data as of now: The fleet panel lists the boats, their location and status and a map: Selecting one boat with the edit button, goes to the boat detail view: You can start the simulation on the ship movement by seleting one of the three pre-defined scenarios: Fire some containers One reefer down * Or boat going thru a heat waves The command is sent to the Simulator and the boat will start to move and generate container metrics: The simulation implementation is yet not completed.","title":"Demonstration"},{"location":"demo/readme/#demo-script","text":"This demo script is using the localhost deployment with a mapping of the host name kcsolution to localhost defined in the /etc/hosts file. For IBM Cloud change the hostname accordingly. Here is how to execute the business process step by step:","title":"Demo Script"},{"location":"demo/readme/#step-1-manufacturer-create-an-order","text":"Go to the http://kcsolution:3110 URL to access the demonstration home page: This page presents the process and some tiles that can be used to simulate the progression within the business process. The grey shadowed tiles are not actives. From the Initiate Orders - Manufacturer we can have the manufacturer creating a new fresh product order to ship over sea. To represent different manufacturer the first select box is used to support different scenarios in the future. GoodManufacturer can be used. Then a list of existing orders may be displayed. You can add order with the UI, but you can also use a script in the order command microservice project: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms/blob/master/order-command-ms/scripts/createOrder.sh Here is an example to add an order to get a voyage from Oakland to Shanghai: ./createOrder.sh localhost:10080 ./orderOacklandToChinaCreateonso There is a lot happening here. The Angular is getting orders using the orders.service.ts service from the BFF at the address: http://localhost:3010/api/orders . The BFF is calling the Order Query Microservice via a javascript client code: getOrders(manuf) function. . The Order Query microservice URL is defined in environment variable or defaulted in the config file. It is mapped to the deployed Order service. (e.g. http://ordercmd:9080/orders) Selecting one order using the Arrow icon open the details of the order: As illustrated in the CQRS diagram: the creation of the order goes to the order command microservice which publishes a OrderCreated event to the orders topic and then consumes it to persist the data to its database. See source code here If you plug a orders consumer you can see the following trace wiht the status of the order being pending and the type of event being OrderCreated {\"payload\":{ \"orderID\":\"1fcccdf2-e29d-4b30-8e52-8116dc2a01ff\", \"productID\":\"Carrot\", \"customerID\":\"GoodManuf\", \"quantity\":10000, \"pickupAddress\": \"...\", \"expectedDeliveryDate\":\"2019-03-31T13:30Z\", \"status\":\"pending\"}, \"type\":\"OrderCreated\", \"version\":\"1\"}","title":"Step 1: Manufacturer create an order:"},{"location":"demo/readme/#step-2-k-container-shipment-manager-looking-at-orders","text":"From the home page goes to the Shipment Inc tile: Then the home page lists the current order the shipment company received The status of those events will be modified over time while the order is processed down stream by the voyage and container services. The following sequence diagram illustrates the flow: Looking at the traces in the voyage service voyages_1 | emitting {\"timestamp\":1548788544290,\"type\":\"OrderAssigned\",\"version\":\"1\",\"payload\":{\"voyageID\":100,\"orderID\":\"1fcccdf2-e29d-4b30-8e52-8116dc2a01ff\"}} or at the orders topic: {\"timestamp\":1548792921679, \"type\":\"OrderAssigned\",\"version\":\"1\", \"payload\":{\"voyageID\":100,\"orderID\":\"1fcccdf2-e29d-4b30-8e52-8116dc2a01ff\"}}","title":"Step 2: K Container Shipment Manager looking at Orders"},{"location":"demo/readme/#step3-simulate-the-ship-in-blue-water","text":"From the home page goes to the Simulate the bluewater tile, then in the main page select one of the available fleet. Only the North Pacific has data as of now: The fleet panel lists the boats, their location and status and a map: Selecting one boat with the edit button, goes to the boat detail view: You can start the simulation on the ship movement by seleting one of the three pre-defined scenarios: Fire some containers One reefer down * Or boat going thru a heat waves The command is sent to the Simulator and the boat will start to move and generate container metrics: The simulation implementation is yet not completed.","title":"Step3: Simulate the ship in blue water"},{"location":"deployments/icp/","text":"Prepare IBM Cloud Private to run the solution","title":"Run on IBM Cloud Private"},{"location":"deployments/icp/#prepare-ibm-cloud-private-to-run-the-solution","text":"","title":"Prepare IBM Cloud Private to run the solution"},{"location":"deployments/iks/","text":"IKS Deployment Prepare IBM Cloud Services to run the solution IBM Cloud offers a set of services to run part of your event driven architecture. We are using the following services for our reference implementation: Kubernetes Service Streaming Analytics Service * Event Streams At the high level the deployed solution will look like the following: Pre-requisites Create an account on IBM Cloud . Install the following CLIs: Docker CLI IBM Cloud CLI IBM Cloud Kubernetes Service plug-in using the following command: sh $ ibmcloud plugin install container-service -r Bluemix Kubernetes CLI IBM Cloud Container Registry plug-in sh $ ibmcloud plugin install container-registry -r Bluemix The following diagram illustrates the command lines interface and how they interact with IBM Cloud components: All our docker images for this solution are in public docker registry: dockerhub under the ibmcase namespace . This means using our images may work for your deployment if you configure the secrets and other configuration files to point to your services (see detail in this section ) It is recommended that you use your own private image repository, so the following section should be performed. Define an image private repository Use the docker container image private registry to push your images and then deploy them to IBM Kubernetes Service. When deploying enterprise application it is strongly recommended to use private registry to protect your images from being used and changed by unauthorized users. Private registries must be set up by the cluster admin to ensure that the credentials to access the private registry are available to the cluster users. In the Catalog Use the Containers category and Container Registry tile. Create the repository with the create button. You can share a repository for multi IKS clusters. Once you access your registry, create a namespace for your solution. We used ibmcaseeda . We will use this namespace when tagging the docker images for our microservice. But first let add a kubernetes cluster. Kubernetes Cluster Service If you need to know more about kubernetes, read the basic concepts here . To create the cluster follow this tutorial . Here is an image of our cluster, with 3 nodes and the smallest configuration: To access to the cluster use the following command: # login to IBM Cloud. Do not need to be done each time. $ ibmcloud login -a https://api.us-east.bluemix.net # Target the IBM Cloud Container Service region in which you want to work. $ ibmcloud cs region-set us-east # You may need to update the CLI, as it changes quite often $ ibmcloud plugin update container-service # Set the KUBECONFIG environment variable. $ export KUBECONFIG=/Users/$USER/.bluemix/plugins/container-service/clusters/fabio-wdc-07/kube-config-wdc07-fabio-wdc-07.yml # Verify you have access to your cluster by listing the node: $ kubectl get nodes To set the cluster config to your cluster use: ibmcloud ks cluster-config <cluster_name_or_ID> As it is recommended to ilosate your deployment from kubernetes default setting, create a namespace that can be the same as the container registry namespace name or something else. Below we create the browncompute namespace: kubectl create namespace browncompute . Event Streams Service on IBM Cloud To provision your service, go to the IBM Cloud Catalog and search for Event Streams . It is in the Integration category. Create the service and specify a name, a region, and a space. In the service credentials create new credentials to get the Kafka broker list, the admim URL and the api_key needed to authenticate the consumers or producers. We will use a kubernetes secret to define the api key (see detail in this section ) * In the Manage panel add the topics needed for the solution. We need at least the following: Streaming Analytics Service The documentation located here describes how to configure the IBM Cloud based Streaming Analytics Service and how to build/deploy the example application. Run the solution on IBM Cloud Kubernetes Services The Event streams broker API key is needed to connect any deployed consumers or producers within kubernetes cluster to access the service in IBM Cloud. To avoid sharing the key with public github we propose to define a kubernetes secret and deploy it to the IKS cluster. Define a Event Stream API key secret: To use Event Streams, we need to get the API key and configure a secret to the browncompute namespace. $ kubectl create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n browncompute # Verify the secrets $ kubectl get secrets -n browncompute This secret is used by all the solution microservices which are using Kafka. The detail of how we use it with environment variables, is described in one of the project here Define a secret to access the IBM CLoud docker image private repository so when your IKS instance accesses docker images it will authenticate. This is also mandatory when registry and clusters are not in the same region. kubectl get secret bluemix-default-secret-regional -o yaml | sed 's/default/browncompute/g' | kubectl -n browncompute create -f - Verify the secret $ kubectl get secrets -n browncompute You will see something like below. NAME TYPE DATA AGE bluemix-browncompute-secret-regional kubernetes.io/dockerconfigjson 1 22m default-token-ggwl2 kubernetes.io/service-account-token 3 41m eventstreams-apikey Opaque 1 24m Now for each microservice of the solution, we have defined a helm chart or a script to deploy it to IKS. 3. Push images to your IBM Cloud private image repository. If not connected to IBM Cloud do the following: $ ibmcloud login -a https://api.us-east.bluemix.net # Target the IBM Cloud Container Service region in which you want to work. $ ibmcloud cs region-set us-east # Set the KUBECONFIG environment variable. $ export KUBECONFIG=/Users/$USER/.bluemix/plugins/container-service/clusters/fabio-wdc-07/kube-config-wdc07-fabio-wdc-07.yml # Verify you have access to your cluster by listing the node: $ kubectl get nodes Then execute the script: ./scripts/pushToPrivate 4. Deploy the helm charts for each components using the scripts/deployHelms . 5. Verify the deployments and pods: $ kubectl get deployments -n browncompute NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE fleetms-deployment 1 1 1 1 23h kc-ui 1 1 1 1 18h ordercommandms-deployment 1 1 1 1 1d orderqueryms-deployment 1 1 1 1 23h voyagesms-deployment 1 1 1 1 19h $ kubectl get pods -n browncompute NAME READY STATUS RESTARTS AGE fleetms-deployment-564698b998-7pb2n 1/1 Running 0 23h kc-ui-749d7df9db-jl6tv 1/1 Running 0 14h ordercommandms-deployment-d6dc4fdc7-5wjtp 1/1 Running 0 1d orderqueryms-deployment-5db96455f-6fqp5 1/1 Running 0 23h voyagesms-deployment-6d7f8cdc8d-hnvq6 1/1 Running 0 19h You can perform a smoke test with the scripts/smokeTests . Access the kubernetes console from your IKS deployment to see the deployment","title":"Run on IBM Cloud Kubernetes Services"},{"location":"deployments/iks/#iks-deployment","text":"","title":"IKS Deployment"},{"location":"deployments/iks/#prepare-ibm-cloud-services-to-run-the-solution","text":"IBM Cloud offers a set of services to run part of your event driven architecture. We are using the following services for our reference implementation: Kubernetes Service Streaming Analytics Service * Event Streams At the high level the deployed solution will look like the following:","title":"Prepare IBM Cloud Services to run the solution"},{"location":"deployments/iks/#pre-requisites","text":"Create an account on IBM Cloud . Install the following CLIs: Docker CLI IBM Cloud CLI IBM Cloud Kubernetes Service plug-in using the following command: sh $ ibmcloud plugin install container-service -r Bluemix Kubernetes CLI IBM Cloud Container Registry plug-in sh $ ibmcloud plugin install container-registry -r Bluemix The following diagram illustrates the command lines interface and how they interact with IBM Cloud components: All our docker images for this solution are in public docker registry: dockerhub under the ibmcase namespace . This means using our images may work for your deployment if you configure the secrets and other configuration files to point to your services (see detail in this section ) It is recommended that you use your own private image repository, so the following section should be performed.","title":"Pre-requisites"},{"location":"deployments/iks/#define-an-image-private-repository","text":"Use the docker container image private registry to push your images and then deploy them to IBM Kubernetes Service. When deploying enterprise application it is strongly recommended to use private registry to protect your images from being used and changed by unauthorized users. Private registries must be set up by the cluster admin to ensure that the credentials to access the private registry are available to the cluster users. In the Catalog Use the Containers category and Container Registry tile. Create the repository with the create button. You can share a repository for multi IKS clusters. Once you access your registry, create a namespace for your solution. We used ibmcaseeda . We will use this namespace when tagging the docker images for our microservice. But first let add a kubernetes cluster.","title":"Define an image private repository"},{"location":"deployments/iks/#kubernetes-cluster-service","text":"If you need to know more about kubernetes, read the basic concepts here . To create the cluster follow this tutorial . Here is an image of our cluster, with 3 nodes and the smallest configuration: To access to the cluster use the following command: # login to IBM Cloud. Do not need to be done each time. $ ibmcloud login -a https://api.us-east.bluemix.net # Target the IBM Cloud Container Service region in which you want to work. $ ibmcloud cs region-set us-east # You may need to update the CLI, as it changes quite often $ ibmcloud plugin update container-service # Set the KUBECONFIG environment variable. $ export KUBECONFIG=/Users/$USER/.bluemix/plugins/container-service/clusters/fabio-wdc-07/kube-config-wdc07-fabio-wdc-07.yml # Verify you have access to your cluster by listing the node: $ kubectl get nodes To set the cluster config to your cluster use: ibmcloud ks cluster-config <cluster_name_or_ID> As it is recommended to ilosate your deployment from kubernetes default setting, create a namespace that can be the same as the container registry namespace name or something else. Below we create the browncompute namespace: kubectl create namespace browncompute .","title":"Kubernetes Cluster Service"},{"location":"deployments/iks/#event-streams-service-on-ibm-cloud","text":"To provision your service, go to the IBM Cloud Catalog and search for Event Streams . It is in the Integration category. Create the service and specify a name, a region, and a space. In the service credentials create new credentials to get the Kafka broker list, the admim URL and the api_key needed to authenticate the consumers or producers. We will use a kubernetes secret to define the api key (see detail in this section ) * In the Manage panel add the topics needed for the solution. We need at least the following:","title":"Event Streams Service on IBM Cloud"},{"location":"deployments/iks/#streaming-analytics-service","text":"The documentation located here describes how to configure the IBM Cloud based Streaming Analytics Service and how to build/deploy the example application.","title":"Streaming Analytics Service"},{"location":"deployments/iks/#run-the-solution-on-ibm-cloud-kubernetes-services","text":"The Event streams broker API key is needed to connect any deployed consumers or producers within kubernetes cluster to access the service in IBM Cloud. To avoid sharing the key with public github we propose to define a kubernetes secret and deploy it to the IKS cluster. Define a Event Stream API key secret: To use Event Streams, we need to get the API key and configure a secret to the browncompute namespace. $ kubectl create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n browncompute # Verify the secrets $ kubectl get secrets -n browncompute This secret is used by all the solution microservices which are using Kafka. The detail of how we use it with environment variables, is described in one of the project here Define a secret to access the IBM CLoud docker image private repository so when your IKS instance accesses docker images it will authenticate. This is also mandatory when registry and clusters are not in the same region. kubectl get secret bluemix-default-secret-regional -o yaml | sed 's/default/browncompute/g' | kubectl -n browncompute create -f - Verify the secret $ kubectl get secrets -n browncompute You will see something like below. NAME TYPE DATA AGE bluemix-browncompute-secret-regional kubernetes.io/dockerconfigjson 1 22m default-token-ggwl2 kubernetes.io/service-account-token 3 41m eventstreams-apikey Opaque 1 24m Now for each microservice of the solution, we have defined a helm chart or a script to deploy it to IKS. 3. Push images to your IBM Cloud private image repository. If not connected to IBM Cloud do the following: $ ibmcloud login -a https://api.us-east.bluemix.net # Target the IBM Cloud Container Service region in which you want to work. $ ibmcloud cs region-set us-east # Set the KUBECONFIG environment variable. $ export KUBECONFIG=/Users/$USER/.bluemix/plugins/container-service/clusters/fabio-wdc-07/kube-config-wdc07-fabio-wdc-07.yml # Verify you have access to your cluster by listing the node: $ kubectl get nodes Then execute the script: ./scripts/pushToPrivate 4. Deploy the helm charts for each components using the scripts/deployHelms . 5. Verify the deployments and pods: $ kubectl get deployments -n browncompute NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE fleetms-deployment 1 1 1 1 23h kc-ui 1 1 1 1 18h ordercommandms-deployment 1 1 1 1 1d orderqueryms-deployment 1 1 1 1 23h voyagesms-deployment 1 1 1 1 19h $ kubectl get pods -n browncompute NAME READY STATUS RESTARTS AGE fleetms-deployment-564698b998-7pb2n 1/1 Running 0 23h kc-ui-749d7df9db-jl6tv 1/1 Running 0 14h ordercommandms-deployment-d6dc4fdc7-5wjtp 1/1 Running 0 1d orderqueryms-deployment-5db96455f-6fqp5 1/1 Running 0 23h voyagesms-deployment-6d7f8cdc8d-hnvq6 1/1 Running 0 19h You can perform a smoke test with the scripts/smokeTests . Access the kubernetes console from your IKS deployment to see the deployment","title":"Run the solution on IBM Cloud Kubernetes Services"},{"location":"deployments/local/","text":"Run locally To run the full solution locally you can use a kubernetes cluster like Minikube or Docker Edge, or use docker-compose. We propose to use docker-compose for local deployment, and here are the instructions to launch the backbone (kafka and zookeeper) and the solution components: Get docker and install it (if not done yet) Get docker compose Assign at least 8GB of memory and may be 4 to 6 CPUs to your docker runtime. This is set in the Preferences menu and under the Advanced tab. In one Terminal window use our compose file to start the backend components: $ cd docker && docker-compose -f backbone-compose.yml up . The first time the backend is started, you need to configure the Kafka topics we are using in the solution. The local script: scripts/createLocalTopics.sh will create them. Note: Starting those components will create two folders in the docker folder: kafka1 and zookeeper1. Those folders could be deleted to restart from a fresh environment. In a second terminal use our second compose file to start the web server and the other microservices: $ docker-compose -f kc-solution-compose.yml up Verify the different components work fine. You can use the different test scripts we have defined in each of the microservices or use the following URLs: For the user interface URL http://localhost:3010 The Fleet Simulator API URL or one of its operation to get the fleet names: http://localhost:9080/fleetms/fleets. Add an order with the `scripts/ in the refarch-kc-orders/ms/ The voyages http://localhost:3100/voyage The order query microservice http://localhost:11080/orders/byManuf/GoodManuf Read the demo script to see how all those components work together to present the business process. Build This project includes some scripts to help build the full solution once all the repositories are cloned. If you have some problem during this integrated build we recommend going into each project to assess the build process in detail. Also for development purpose, going into each project, you can learn how to build and run locally. To be able to build you need npm, node, maven and docker: Get docker and install it (if not done yet). Get maven and add it to your PATH Get node and npm Build all projects in one command by executing: scripts/buildAll Use the scripts/imageStatus script to verify your docker images are built: ibmcase/kc-ui latest c89827424689 15 hours ago 596MB registry.ng.bluemix.net/ibmcaseeda/kc-ui latest c89827424689 15 hours ago 596MB ibmcase/kc-orderqueryms latest 09406c8795e8 23 hours ago 548MB registry.ng.bluemix.net/ibmcaseeda/kc-orderqueryms latest 09406c8795e8 23 hours ago 548MB ibmcase/kc-ordercmdms latest 5190db45e4bf 23 hours ago 548MB registry.ng.bluemix.net/ibmcaseeda/kc-ordercmdms latest 5190db45e4bf 23 hours ago 548MB ibmcase/kc-voyagesms latest 54b8d6a61f4e 23 hours ago 1.16GB registry.ng.bluemix.net/ibmcaseeda/kc-voyagesms latest 54b8d6a61f4e 23 hours ago 1.16GB ibmcase/kc-fleetms latest a5e1d40a8b1f 23 hours ago 616MB registry.ng.bluemix.net/ibmcaseeda/kc-fleetms latest a5e1d40a8b1f 23 hours ago 616MB If you want to delete the docker images after that use the command: docker rmi $(docker -aq | grep ibmcase)","title":"Run locally"},{"location":"deployments/local/#run-locally","text":"To run the full solution locally you can use a kubernetes cluster like Minikube or Docker Edge, or use docker-compose. We propose to use docker-compose for local deployment, and here are the instructions to launch the backbone (kafka and zookeeper) and the solution components: Get docker and install it (if not done yet) Get docker compose Assign at least 8GB of memory and may be 4 to 6 CPUs to your docker runtime. This is set in the Preferences menu and under the Advanced tab. In one Terminal window use our compose file to start the backend components: $ cd docker && docker-compose -f backbone-compose.yml up . The first time the backend is started, you need to configure the Kafka topics we are using in the solution. The local script: scripts/createLocalTopics.sh will create them. Note: Starting those components will create two folders in the docker folder: kafka1 and zookeeper1. Those folders could be deleted to restart from a fresh environment. In a second terminal use our second compose file to start the web server and the other microservices: $ docker-compose -f kc-solution-compose.yml up Verify the different components work fine. You can use the different test scripts we have defined in each of the microservices or use the following URLs: For the user interface URL http://localhost:3010 The Fleet Simulator API URL or one of its operation to get the fleet names: http://localhost:9080/fleetms/fleets. Add an order with the `scripts/ in the refarch-kc-orders/ms/ The voyages http://localhost:3100/voyage The order query microservice http://localhost:11080/orders/byManuf/GoodManuf Read the demo script to see how all those components work together to present the business process.","title":"Run locally"},{"location":"deployments/local/#build","text":"This project includes some scripts to help build the full solution once all the repositories are cloned. If you have some problem during this integrated build we recommend going into each project to assess the build process in detail. Also for development purpose, going into each project, you can learn how to build and run locally. To be able to build you need npm, node, maven and docker: Get docker and install it (if not done yet). Get maven and add it to your PATH Get node and npm Build all projects in one command by executing: scripts/buildAll Use the scripts/imageStatus script to verify your docker images are built: ibmcase/kc-ui latest c89827424689 15 hours ago 596MB registry.ng.bluemix.net/ibmcaseeda/kc-ui latest c89827424689 15 hours ago 596MB ibmcase/kc-orderqueryms latest 09406c8795e8 23 hours ago 548MB registry.ng.bluemix.net/ibmcaseeda/kc-orderqueryms latest 09406c8795e8 23 hours ago 548MB ibmcase/kc-ordercmdms latest 5190db45e4bf 23 hours ago 548MB registry.ng.bluemix.net/ibmcaseeda/kc-ordercmdms latest 5190db45e4bf 23 hours ago 548MB ibmcase/kc-voyagesms latest 54b8d6a61f4e 23 hours ago 1.16GB registry.ng.bluemix.net/ibmcaseeda/kc-voyagesms latest 54b8d6a61f4e 23 hours ago 1.16GB ibmcase/kc-fleetms latest a5e1d40a8b1f 23 hours ago 616MB registry.ng.bluemix.net/ibmcaseeda/kc-fleetms latest a5e1d40a8b1f 23 hours ago 616MB If you want to delete the docker images after that use the command: docker rmi $(docker -aq | grep ibmcase)","title":"Build"},{"location":"deployments/reposlist/","text":"Related repositories This solution supports a set of related repositories which includes user interface, a set of microservices to implement the Event Sourcing and CQRS patterns, and to implement simulators and analytics content. In each repository we are explaining the design and implementation approach, how to build and run them for development purpose. The command ./scripts/clone.sh in this project clones all the dependant repositories as part of the solution. User Interface in Angular 7 and Backend For Frontend server used for demonstration purpose . Ship and fleet microservice of this solution are grouped in one repository. We may change that later if we need it. Real time analytics with IBM Streaming Analytics to identify problem on containers from real time events. Order management microservice using CQRS and event sourcing pattern . Voyage microservice to support the order management and ship voyage assignment.","title":"Related repositories"},{"location":"deployments/reposlist/#related-repositories","text":"This solution supports a set of related repositories which includes user interface, a set of microservices to implement the Event Sourcing and CQRS patterns, and to implement simulators and analytics content. In each repository we are explaining the design and implementation approach, how to build and run them for development purpose. The command ./scripts/clone.sh in this project clones all the dependant repositories as part of the solution. User Interface in Angular 7 and Backend For Frontend server used for demonstration purpose . Ship and fleet microservice of this solution are grouped in one repository. We may change that later if we need it. Real time analytics with IBM Streaming Analytics to identify problem on containers from real time events. Order management microservice using CQRS and event sourcing pattern . Voyage microservice to support the order management and ship voyage assignment.","title":"Related repositories"},{"location":"design/architecture/","text":"Architecture System Context When dealing with architecture we want to start by high level and drill down into more detail view. The system context view for the solution looks like the diagram below: Components view Deploying the different components using event-driven and microservice patterns, we may organize them as in the following figure where event backbone ensures pub/sub implementation and supports the event sourcing: Top left represents the user interface to support the demonstration of the K.Container solution, with a set of widgets to present the ships movements, the container tracking / monitoring and the event dashboards. The botton of the UI will have controls to help performaing the step by step demonstration. The event backbone is used to define a set of topics used in the solution and as event sourcing for microservice to microservice data eventual consistency support. Each service supports the top-level process with context boundary defining the microservice scope. Streaming analytics is used to process aggregates and analytics on containers and ships movement data coming in real time. As we develop by iterations the current scope of the Minimum Viable Product is only addressing the following components: Read more on EDA design pattern... The 12 factors application is also used for order management microservice. Summary of microservice scopes for shipment handling: As presented in the note about event driven microservice patterns , we are using a set of event-driven design patterns to develop this solution. One of them is the sub domain decomposition . From the analysis output we have the aggregates, actors and data that are helping us to extract a set of subdomains. Fleet Service : responsibles to group the ship (container carriers), in fleet, per major ocean. Information model: Fleet has multiple ships, Ship has unique identifier (we will use its name), and a container capacity (represented as a matrix to make it simple), current position, status, voyage identifier for the voyage it is doing. Events: Ship commission, ship position, load container event, unload container event, start itinerary X, arrive at port, docked,... Operations: getFleets, get ships in a fleet, get ship by ID. CRUD Fleet and Ship. Implementation in this project. Voyages Service : define a set of voyage schedules supported by the shipping company Information model: voyageID, shipID, src_Port, planned_departure_date, dest_port, planned_arrival_dates, free_space_this_leg Events: add itinerary route, OrderAssigned Operations: CRUD on itinerary routes, query on capacity availability, assign slot to order, free slot for order. Implementation in this project. Order Service : manage the shipment order Information model: Booking id , customer, pickup location, pickup after date, deliver location, expected deliver date, order status, assigned container Events: order placed, order assigned to voyage( sets VoyageID, ship ID ), container assigned to order ( Sets container ID), Landorder, Transport associated with pickup container, Order status event, Order billing/accounting event Operations: CRUD on order, update order status Implementation in this project. Container Service : Information model: Container Id, Container temperature, container position, container condition ( maintenance goods), current associated order Events: Operations: CRUD on container (not yet implemented) Customs and Export Service (not yet implemented) Land Transport Service: (not yet implemented)","title":"Architecture"},{"location":"design/architecture/#architecture","text":"","title":"Architecture"},{"location":"design/architecture/#system-context","text":"When dealing with architecture we want to start by high level and drill down into more detail view. The system context view for the solution looks like the diagram below:","title":"System Context"},{"location":"design/architecture/#components-view","text":"Deploying the different components using event-driven and microservice patterns, we may organize them as in the following figure where event backbone ensures pub/sub implementation and supports the event sourcing: Top left represents the user interface to support the demonstration of the K.Container solution, with a set of widgets to present the ships movements, the container tracking / monitoring and the event dashboards. The botton of the UI will have controls to help performaing the step by step demonstration. The event backbone is used to define a set of topics used in the solution and as event sourcing for microservice to microservice data eventual consistency support. Each service supports the top-level process with context boundary defining the microservice scope. Streaming analytics is used to process aggregates and analytics on containers and ships movement data coming in real time. As we develop by iterations the current scope of the Minimum Viable Product is only addressing the following components: Read more on EDA design pattern... The 12 factors application is also used for order management microservice.","title":"Components view"},{"location":"design/architecture/#summary-of-microservice-scopes-for-shipment-handling","text":"As presented in the note about event driven microservice patterns , we are using a set of event-driven design patterns to develop this solution. One of them is the sub domain decomposition . From the analysis output we have the aggregates, actors and data that are helping us to extract a set of subdomains. Fleet Service : responsibles to group the ship (container carriers), in fleet, per major ocean. Information model: Fleet has multiple ships, Ship has unique identifier (we will use its name), and a container capacity (represented as a matrix to make it simple), current position, status, voyage identifier for the voyage it is doing. Events: Ship commission, ship position, load container event, unload container event, start itinerary X, arrive at port, docked,... Operations: getFleets, get ships in a fleet, get ship by ID. CRUD Fleet and Ship. Implementation in this project. Voyages Service : define a set of voyage schedules supported by the shipping company Information model: voyageID, shipID, src_Port, planned_departure_date, dest_port, planned_arrival_dates, free_space_this_leg Events: add itinerary route, OrderAssigned Operations: CRUD on itinerary routes, query on capacity availability, assign slot to order, free slot for order. Implementation in this project. Order Service : manage the shipment order Information model: Booking id , customer, pickup location, pickup after date, deliver location, expected deliver date, order status, assigned container Events: order placed, order assigned to voyage( sets VoyageID, ship ID ), container assigned to order ( Sets container ID), Landorder, Transport associated with pickup container, Order status event, Order billing/accounting event Operations: CRUD on order, update order status Implementation in this project. Container Service : Information model: Container Id, Container temperature, container position, container condition ( maintenance goods), current associated order Events: Operations: CRUD on container (not yet implemented) Customs and Export Service (not yet implemented) Land Transport Service: (not yet implemented)","title":"Summary of microservice scopes for shipment handling:"},{"location":"design/readme/","text":"From Analysis to Microservice Specifications Goals and outline of section This section describes the design step which uses output from the event storming session and subsequent analysis and derives a set of micro services design specifications. The goals for the design step and the resulting specifications are: To support highly modular cloud native microservices. To adopt event coupled microservices - facilitating independent modification and evolution of each microservice separately. To allow applying event-driven patterns such as event sourcing, CQRS and SAGA. Since, we are delivering a demonstration application there will be some simulator / scaffolding / testing services mixed in with the required business processing. This is a common occurrence in agile development and it may be helpful to show how decision to scope and simplify a particular build and test step interacts with decisions relating strictly to microservices design. Requirements for scalability, coupling of microservices only through the event back bone and eventual consistency differentiate this step from previous Event Storming and Domain Driven Design activities which were 100% business requirement driven. Starting materials generated during Event Storming and Analysis We make use of the following materials generated during Event Storming and analysis of the Container Shipment example problem: Event Sequence flow. Events \u2013 business description. Critical events. Aggregates and services: Users \u2013 role based user stories. Commands. Event linkages. Policies. Event prediction and probability flows. Conceptual Data Model. The derivation of these material was described in: Analysis . Event linked microservices design - structure A complete microservices specification (the target of this design step) includes specifications of the following: Event Topics Used to configure the Kafka Event Backbone Event types within each event topic Microservices: These are finer grained than aggregates May separate query and command; possibly multiple queries May Separate whether simulation or business processing component Demonstration Control \u2013 main User Interface Scaffolding and testing services - whether local and cloud versions Microservice specification Data within Each microservice APIs ( Synchronous ) Topics and events Subscribed to Events published / emitted List of end to end interactions List of logic segments per microservice Recovery processing, scaling We expect this to be highly patterned and template driven not requiring example-specific design With the above information coding of each microservice and other components within a \"sprint\" should be straightforward. Steps in the design process Here we describe in generic terms, each step in the process of deriving event-linked microservice specifications. In following section we will describe in more detail how each of these steps plays out in the specific context of the container shipment example. List of generic steps: Step 1: Limit the context and scope for this particular build / sprint We assume that we are developing a particular build for a sprint within some agile development approach, deferring additional functions and complexity to later sprints: Working from the initial list of aggregates, select which aggregates will be included in this build For each aggregate possible choices are: (1) to completely skip and workaround the aggregate in this build (2) to include a full lifecycle implementation of the aggregate (3) to provide a simplified lifecycle implementation - typically a table of entities is initialized at start up, and state changes to existing entities are tracked Determine whether there are simulation services or predictive analytics service to be included in the build Identify the external query APIs and command APIs which this build should support Create entity lifecycle diagrams for entites having a full lifecycle implementation in this build / sprint. Step 2: Identify specific microservices in each aggregate Each aggregate will be implemented as some composition of: (1) a command microservice managing state changes to the entities in this aggregate (2) possibly one or more separate (CQRS) query services providing internal or external API query capabilities (3) additional simulation, predictive analytics or User Interface microservices The command microservice will be built around and manage a collection of active entites for the aggregate, keyed by some primary key The separation of each aggregate into specific component microservices as outlined above, will be a complete list of microservices for the build / sprint. Identify the data collections, and collection organization (keying structure) in each command and query microservice for this build. Step 3: Generate microservice interaction diagrams for the build The diagram will show API calls initiating state change. It shows for each interaction whether this is a synchronous API calls or an asynchronous event interaction via the event backbone. The diagram labels each specific event interaction between microservices trigerring a state change. Typically queries are synchronous API calls since the caller cannot usefully proceeed until a result is returned. From this we can extract: (1) a complete list of event types on each topic, with information passed on each event type. (2) the complete list of \u201clogic segments\u201d for each microservice processing action in response to an API call or initiating event. When, at the next level of detail, the individual fields in each event are specified and typed, the CloudEvents standard should be assumed as a starting point. Step 4: Specify recovery approach in case a microservice fails If a microservice fails it will need to recover its internal state by resubscribing to one or more topics on the event backbone. In general, command and query microservices will have a standard pattern for doing this. Any custom event filtering and service specific logic should be specified. Concepts and rationale underlying the design approach What is the difference between event information stored in the event backbone and state data stored in the microservices? The event information stored persistently in the event backbone is organized by topic and, within each topic, entirely by event time-of-occurrence. While the state information in a microservice is a list (collection) of all currently active entities of the owning aggregate (e.g. all orders, all voyages etc) and the current state of each such entity. The entity records are keyed by primary key, like an OrderID. While implementing microservice using event sourcing, CQRS, the persisted entity records are complementary to the historically organized information in the event backbone. When is it acceptable to be using synchronous interactions between services instead of asynchronous event interacts through the event backbone? For non-state-changing queries, for which the response is always instantaneously available a synchronous query call may be acceptable and will provide a simpler more understandable interface. Any processing which can be though of as being triggered by some state change in another aggregate should be modelled with an asynchronous event, because as the solution evolves other new microservices may also need to be aware of such event. We do not want to have to go back and change logic existing service where this event originated to have that microservice actively report the event to all potential consumers. How do we save microservices from having to maintain data collections with complex secondary indexing for which eventual consistency will be hard to implement? Each command microservice should do all its state changing updates using the primary key lookup only for its entities. Each asynchronous event interaction between microservices should carry primary entityIds ( orderID, VoyageID, shipID) for any entities associated with the interaction. Each query which might require speciaoized secondary indexing to respond to queries can be implemented in a separate CQRS query service which subscribes to events to do all internal updating and receives events from the event backbone in a ( Consistent) eventually correct order. This allows for recovery of any failed service by rebuilding it in \"eventually correct\" order. Specific application to Container Shipment example In this section we discuss how the generic steps introduced in previous section can be applied for the Container shipping example: Step1: Context and scope for demonstration build An initial scoping decision is that the demonstration will address shipment orders and shipment progress initiated by the \"manufacturer\" of the fresh goods with the shipment company. In the context of the example there is also discussion of manufacturer and retailer reaching some agreement on the goods to be delivered but this is not part of the demonstrated capabilities. The Event Storming analysis of the shipment problem was end-to-end and involved many aggregates including: Orders, Voyages, Trucking operations both at the source (manufacturer pickup) and at the destination (retailer delivery), Customs and export interactions, Container loading into ship at source port and unloading from ship at destination port, containers and fleet of ships. To have a simple initial demonstration build showing the role of event-driven architecture and event coupled microservices, as an initial step towards development of a more complete system using agile incremental development and deployment, the initial demonstration build makes the following simplifications and scoping decisions: This build will have no implementation of: Trucking operations, Customs and export, or Dockside Handling aggregates It will show a full lifecyle for a manufacturer user to place an order for shipment, seeing a filled container placed on board ship transported to the destination port and delivered. It will include a simulation service for ship movements - tracking the movement of ships carrying containers It will include simulation Container metrics such as temperature, current consumption while onboard ship It will provide a query for a user to track an order and the current location and state of the associated shipment It will include a real time analytic solution to assess problems happening within containers Based on the scope selection above, active aggregates in the build will be: Orders - with support for a complete order lifecycle Voyages - list of planned port to port passges with dates and manifests for each sip Containers - with allocation of a container to each order and temperature tracking of refrigerated containers Ships - with tracking of current voyage and current geographical position of each container ship The event backbone will be configured with a topic for each of the above aggregated. We expect to see multiple event types on each topic, but subscriptions and sequencing of events will be within these high level topics. Command APIs will be provided to: Place a new shipment order. Track an existing order, to confirm its booking state or to resolve the actual location and status of the container in transit. Modify an order request which could not be booked within the requested time window. A more complete and complex build could include an API for a shipping company person to optimally and manually assign orders to voyages, but for this initial demonstration we will automate this process and assign orders automatilly to the first voyage found meeting the requested requirements. Specifically, each order is assigned to the first located voyage: Going from the port nearest to pickup location To the port nearest the delivery location, Within the requested time window for pickup and delivery With available capacity for an additional container on that voyage. Additional APIs will be need: To initiate the overall demonstration To manage and view specific simulation component - container simulation and analytics and ship simulation and analytics. Shipment order lifecycle and state change events The scoping decisions for the demonstration build listed above are reflected in a shipment order life cycle diagram shown below. A shipment order is initially created with an API call made by a manufacturer, or via a user interface (See demonstration script ). The order request specifies: The pickup location where empty container will be loaded The delivery location where the container is to be delivered to (we expect this to be in a remote country requiring a sea voyage) The shipment time window i.e.: Earliest date at which goods are available at pickup location for loading into the container Date by which delivery to the destination address is required Since our initial demonstration build expects to show refrigeration behavior and track preservation of a cold chain, we assume that orders are for some commodity which requires refrigeration during its shipment. A graphical view of this API with some additional field specification is provided in: When a new shipment order is placed, the shipping company must determine whether there is available capacity in some planned ship voyage which meets all the requirements specified by the manufacturer / customer. If there is a planned voyage with available capacity for additional container going from the source port nearest the pickup location to the destination port nearest to the delivery location then the order can transition to state=BOOKED and positive confirmation of the order returned to the requester. If no such voyage is available then the shipment order transitions to state=REJECTED (No Availability) and this is reported back to the requester. Once an order is BOOKED, then the expected dates and locations where for which a container will be needed are known. A request can be issued to book a specific (refrigerated) container for use with this shipment. We assume that the shipping company always has enough container available to meet expected shipment demand, hence the shipment order will transition to state=CONTAINER_ALLOCATED when this container booking is received. Since the scope for this demonstration build excluded the simulation of trucking operations to get the goods from the manufacturer's pickup location, export clearance and actual dockside loading operations, once an order has a container allocated it is \"ready to go\" and transitions to state=FULL_CONTAINER_VOYAGE_READY. The actual event of recording the container as being on board ship and at sea will not happen until simulated time in the demonstration reaches the scheduled start of the voyage on which that container is booked and the container ship assigned to that voyage is in the source port and also ready to go. At that point in simulated time, the state of the shipment order changes from state = FULL_CONTAINER_VOYAGE_READY to state = CONTAINER_ON_SHIP. While the order has state = CONTAINER_ON_SHIP, then we will be receiving temperature information from the Container simulation and Ship position information from the ship simulation service. Both provide a continuous streaming souces of information which should be considered part of the extended shipment state. After some period of simulated time, the ship will reach the destination port of the voyage. At this time the order transitions to state = CONTAINER_OFF_SHIP since our scope excluded simulation of actual dockside unloading information. Since we are not modelling customs clearance or trucking operations, there are no further events to be modeled until the order state = CONTAINER_DELIVERED. Since we are not modelling invoicing and billing operations the Container can be deallocated from this order and returned to some pool of free containers. When that has occurred the order state can be considered state = ORDER_COMPLETED. We have described the nornal, exception-free path first. There are two exception cases modelled: At the time a new shipment order is requested, there may be no voyage with available capacity meeting the location and time requirements of the request. When this occurs, the manufacturer/user is informed and the order state becomes state= REJECTED (No Availability). At this point, the user can modify the order with a second API requests changing dates or possibly locations. This retry request could still fail returning the order back to state = REJECTED ( No availability). Alternatively the changes in dates and location could be enough for an available voyage to be found. When this occurs the order will transition to state = BOOKED modified. If an API call to modify an order is made and the order is in some state different from state=REJECTED (No availability), we reject the API request. There could be race conditions, the order is in the process of being assigned to a voyage, or complex recovery issues. What if the order is already in a container and at sea when a modify order is received ? Full treatment of these complex business specific issues is out of scope and avoided by the state check in the modify order call API call We also model the exception condition when the refrigeration unit in a container fails or is misset or over loaded. If the temperature in the container goes outside the service level range for that shipment the goods must be considered spoiled. The order will transition from state = CONTAINER_ON_SHIP to state = ORDER_SPOILED (Temperature out of Range). Some complex business recovery such as compensating the customer and possibly scheduling a replacement shipment may be required. The details will be contract specific and outside the scope, but we do include the use of Streaming event container analytics to detect the spoilage and use rule based real-time /edge adjustments of the refrigeration gear to avoid spoilage in the demonstration simulation. Step 2: microservices and microservice owned data for demonstration build In this step we fix the specific microservices for each aggregate and the data organization for each microservice. Orders Aggregate For Orders we are implementing the CQRS pattern and we will need an orders-command-ms which will maintain a list of all current active orders and the current state of each order. The order state will as described above. The collection of active orders will be keyed by orderID. The orders-command-ms will offer APIs for create order and modify order since these are external interactions. It makes sense to use CQRS and separate out order tracking into a separate oders-query-ms since: The demand for order tracking might have significantly more intense scalability needs than order commands. Orders are typically created once and changes state a handful of times. There could be many different users querying status of a particular orders independently and each requesting tracking multiple times for each order to determine if there is some delay expected. Order state tracking information should probably be organized by requesting customer NOT by order ID: since customers should be allowed to see status on their own orders but not on other customer's orders when the shipping company is tracking an order it is most frequently doing so on behalf of a specific customer With this approach orders-query-ms becomes a CQRS query service with internal state updated from the event backbone, and an order tracking API. Voyages Aggregate For Voyages we will need a voyages-command-ms which will maintain a list of all voyages and their current state. In any given run of the demonstration we will work with a fixed set of voyages - effectively the schedule for the container fleet - so there is no need for an API to create additional voyages. The voyage definition will be read from file when the build is initializing. We expect this voyage data to be well formed: each voyage has a container ship in the fleet allocated to make the voyage the voyages assigned to any one ship are properly \"chained\". For the sequence of voyages assigned to any one container ship, the destination port of the nth voyage is always the start port of the (n+1)th voyage. The states of a voyage are: SCHEDULED - in this state it can accept order bookings, knows how much free space is available for additional bookings, and knows the orderIDs of each shipment already booked on the voyage IN_PROGRESS - in this state it includes a manifest a list of the orderIDs and containerIDs on board the ship COMPLETED - a voyage in the completed state supports tracing continers, may know which containers in the voyage were spoiled etc It will be helpful for the voyage-command-ms to include a query service to lookup voyages with a particular source port and destination port in a particular time windaw. This will help process booking request event but does not need to be an external API hence there is no strong argument for realizing this as a separate CQRS query service. Containers Aggregate For Containers we will use a containers-command-ms to maintain a list of defined containerIDs and track the state of each container. A fixed set of valid container IDs will be initialized at demonstration start time. As noted previously we will assume this to be enough for all requested orders to be assigned a container without availability issues. Since the collection of containers is fixed the component will not need a command API The container current state maintained in container-command-ms is: state = FREE - this container is not in use and is available to be assigned to a new shipment order state = ALLOCATED - this container is allocated to an order orderID and potentially in use for that shipment. We will be modelling and performing streaming analytics on temperature inside a (refrigerated) container. Hence there will be a separate services performing this streaming analytics and simulation: container-streaming-svc. Conceptually, while a container is ALLOCATED to a shipment order with state = CONTAINER_ON_SHIP, its internal temperature and power usage will be maintained as streaming state by the container-streaming-svc. Fleet/Ships Aggregate For Ships we will have a monolithic fleet simulation service providing continuous simulation of ship position for each ship and modelling of ship events. This service will include a UI to enable viewing the positions and states of the ships. It may have a separate UI to control the overall demonstration. There is no requirement for any separate microservice maintining additional information on ship state. Step3: Specifing all interactions in a logic flow for the demonstration build Using the understanding of the event flow from the Event Storming session, the scoping of this build, the list of microservices and data within each microservices developed in the steps above, we can write out in a complete interraction flow. This flow illustrates how the microservices are linked together via the Event backbone using event interractions for all non API interractions between distinct microservices. Command microservice interactions - order create through voyage start with container on board The diagram below shows all command interactions from initial order creation through voyage start. The grey (shaded) columns of processing blocks are organized to show processing by the different command microservices. Column 1 shows processing by the orders-command-ms Column 2 shows processing by the voyages-command-ms Column 3 shows processing by the containers-command-ms and in a later figure by containers-streaming-ms Column 4 shows processing by the fleet/ships-simulator-ms Comments on steps in the command flow: A new shipment order request is initiated with the synchronous createOrder API at top left The orders-command-ms will create a new order record in its tale of active orders and populate it with order details. A NewOrder event is emitted on the Orders Topic. The generated orderID or the new shipment order is returned to the requester in the createOrder response. This enables the requester to query the status of an order and possibly modify the parameters of an unbooked order. The voyages-command-ms subscribes to all newOrder events on the Orders topic and tries to assign each new order to an available voyage: This operation is simplified by internally maintaining some list of vayages organized by port pair ( Starting port - ending port combination) and by time within port pair. Using such a list each voyage matching the port pair requirement of the new order can be checked or available capacity. If a voyage meeting all requirements for the new order is found, a booking event is emitted; if not, a rejected (No availability) event is emitted. A booked event causes state change in both the voyage - available capacity reduced, new order added to bookings - and to the order. We choose to make both booking and rejected (no Availabiity) events on the Orders topic rather than the Voyages topic. The orders-command-ms subscribes to Orders: booking and to Orders: Rejected (no availability) events and updates the current state of the affected order with the received information. For bookings, the current state of order is updated with the booking information including VoyageID and now specific pickup and delivery expected dates A rejected order has its state updated to rejected. This enables the requester to modify the order, suggesting different required dates or locations and trigerring a new search for a voyage meeting the modified requirements. Booked orders now have a specific schedule from the booked voyage of when they will need a container allocated for their use A Containers: needEmpty event is emitted to get a specific container allocated for use by the booked shipment The containers-command-ms subscribes to Containers: needEmpty events and allocates an available container for each one: This microservice is maintaining a list of all containers and their current states. For each incoming needEmpty event, it assigns a free container to that order and emits an Orders: allocatedContainer event specifying the containerID of the allocated container. It is very natural/necessary for the allocation of a container to be reported as an asynchronous event since this may occur at any time before the container is needed, possibly significanly later that he Containers:needEmpty event occurs We make Orders: allocatedContainer an event on the Orders topic since that is the most significant state change which it drives. The orders-command-ms subscribes to all Orders: allocatedContainer events and updates the order current state with its allocated containerID Once an order is booked on a voyage and has a container allocated for it to use, the actual physical process of shipment canbegin at this point. Since the delivery of empty container, loading it with goods at the pick up site, truck operations to get it to dockside etc are out of scope for this build, we can consider the container ready for its voyage at this point. Hence the Voyages:fullContainerReady event is emitted at this point by the orders-command-ms. This event includes the containerID of the allocated container. The voyages-command-ms subscribes to Voyages: fullContainerReady events and uses these to construct a complete manifest - the list of pairs which will travel on this voyage At this point the voyage-command-ms interacts with the fleet/ships-simulation-ms to simulate start of voyage We have shown this in the figure as a synchronous call to getNextVoyageInfo. This could also be handled with one or more event interactions The ship-simulator-ms will update the state of this ship to show the available containers and orders on board It will start the simulation of the ship moving on its course tocomplete the vogage The ship-simulator-ms willemit a Voyages: ShipStartedVoyage event The Voyages-command-ms receives this event and for each order/container in the manifest emits an Orders: ContainerOnShip event The orders-command-ms will subscribe to Orders: ContainerOnShip events and update the current state of each identified order with this information. Command microservice interaction - container on ship at sea through shipment complete The diagram below shows all command interactions from container on ship in voyage through shipment delivered and order completed. As in the previous interaction diagram, the columns with grey/shaded processing blocks show work by (1) orders-command-ms (2) voyages-command-ms (3) containers-command-ms and containers-streaming-ms (4) fleet/ships-simulator service respectively. This diagram starts with containers on board a ship which is sailing on specific voyage and is at sea. The fleet/ships-simulator-ms repeatedly simulated movement of the ship along its course It emits Ships: GPSposition events recording the position of the ship at different points in simulated time. Similarly, while the ship is at sea, the container-streams-svc is continuously simulating temperature within the container and edge monitoring to adjust controls if necessary and to report a cold chain breach in that container if it occurs. This will result in a repeated stream of Containers: tempAndGpsState events reporting the temperature, GPS coordinates and possibly power consumption of the container There could also be on or more Containers: action events to adjust or reset controls of the refrigeration unit in the container These adjustment event are initiated by predictive real-ime analytics on the container state If the temperature in the container goes out of range and there is a cold chain failure, a Containers: temperature Out of Range event is emitted After some period of simulated time tracked by these ship position and container state repeated events, the ship will be simulated as arriving at the destination port of the voyage. The ship-simulator-ms emits a Voyages: ShipEndedVoyage event The voyages-command-ms subscribes to Voyages: ShipEndedVoyage and for each such event, emits Orders: containerOffShip It can do this because the current state record for each voyage includes the manifest of pairs which travelled on that voyage the current state of the voyage is updated to COMPLETED The orders-command-ms subscribes to Orders: containerOffShip and updates the state of all orders which have completed their shipping leg as a result of completion of their booked voyage Now, since simulation of the dockside unloading, customs processes, trucking operation to support deliver are out of scope for this build, we can consider the shipment delivered at this point orders-command-ms emits Orders: containerDelivered and marks this as current state of container With the shipment delivered, there is no further need for a container to be associated with this order; orders-command-ms emits Containers: containerReleased The containers-command-ms subscribes to Containers: containerReleased and marks the current state of the identified container as FREE and available to be allocated to other shipment orders The order-command-ms considers process of the shipment order complete at this point It emits Orders: orderComplete and marks this as the current state of the order A more complete and realistic build would statr invoicing and billing event at this poitn , but this was decided to be out of scope at this point The fleet/ships-simulator-ms will continue at this point to start the next voyage in its planned itenerary and interact with voyages-command-ms to do this this is a cycled repetition of start of voyage interaction discussed previously Query microservice service - CQRS Order and Shipment tracking microservices The diagram below shows all interactions with the shipment tracking microservice. This microservice subscribes to many events carrying required information and supports one or more query APIs for different flavors of order and shipment tracking There could be multiple flavors of order and shipment tracking query APIs supported: Order confirmation query could address orders, bookings, rejections, modified orders etc Shipment state query could cover: container assignment, on board ship, ship position, off ship, delivery, etc * Cold chain certification query could want to augment the above with a full temperature log of the container while in transit and expect reporting on temperature range violations. Since we are using a CQRS approach, and all queries are non state changing, we could combine these multiple query levels into a single microservice or separate them out into separate microservices. If query load is intense there could be multiple instances of each such query microservices with load balancing of user requests. The design of these different flavors query services is essentially the same. The internal state data to respond to queries is obtained by subscribing to the necessary Topics. For cold chain and shipment reporting, this will involve all four topics Orders, Voyages, Containers and Ships. Internally the data will be organized by requester of the order, then by orderID, then current state and possibly summaries of repeated event history. The inteaction diagram 3 above illustrates this organization. For any order and shipment tracking query service there are synchronous APIs offered at one side and subscribed events received at the other to gather required state information from the vent backbone. Topics, event types and the event emit and consumption lists From the interaction diagrams we can compile a list of all event types which will occur in the build and check that they are organized into topics in a way which preserves all essential event sequencing. The diagram below lists the event types and topics, showing emitters ( publishers) and consumers ( subscribers) of each event type. Step4: Data recovery considerations for this demonstration build At this point the pattern for data recovery after a microservice failure is understood in principle, but specific recovery demonstration after a failure is out of scope at this point. Read more on EDA design pattern...","title":"Design considerations and microservice identifications"},{"location":"design/readme/#from-analysis-to-microservice-specifications","text":"","title":"From Analysis to Microservice Specifications"},{"location":"design/readme/#goals-and-outline-of-section","text":"This section describes the design step which uses output from the event storming session and subsequent analysis and derives a set of micro services design specifications. The goals for the design step and the resulting specifications are: To support highly modular cloud native microservices. To adopt event coupled microservices - facilitating independent modification and evolution of each microservice separately. To allow applying event-driven patterns such as event sourcing, CQRS and SAGA. Since, we are delivering a demonstration application there will be some simulator / scaffolding / testing services mixed in with the required business processing. This is a common occurrence in agile development and it may be helpful to show how decision to scope and simplify a particular build and test step interacts with decisions relating strictly to microservices design. Requirements for scalability, coupling of microservices only through the event back bone and eventual consistency differentiate this step from previous Event Storming and Domain Driven Design activities which were 100% business requirement driven.","title":"Goals and outline of section"},{"location":"design/readme/#starting-materials-generated-during-event-storming-and-analysis","text":"We make use of the following materials generated during Event Storming and analysis of the Container Shipment example problem: Event Sequence flow. Events \u2013 business description. Critical events. Aggregates and services: Users \u2013 role based user stories. Commands. Event linkages. Policies. Event prediction and probability flows. Conceptual Data Model. The derivation of these material was described in: Analysis .","title":"Starting materials generated during Event Storming and Analysis"},{"location":"design/readme/#event-linked-microservices-design-structure","text":"A complete microservices specification (the target of this design step) includes specifications of the following: Event Topics Used to configure the Kafka Event Backbone Event types within each event topic Microservices: These are finer grained than aggregates May separate query and command; possibly multiple queries May Separate whether simulation or business processing component Demonstration Control \u2013 main User Interface Scaffolding and testing services - whether local and cloud versions Microservice specification Data within Each microservice APIs ( Synchronous ) Topics and events Subscribed to Events published / emitted List of end to end interactions List of logic segments per microservice Recovery processing, scaling We expect this to be highly patterned and template driven not requiring example-specific design With the above information coding of each microservice and other components within a \"sprint\" should be straightforward.","title":"Event linked microservices design - structure"},{"location":"design/readme/#steps-in-the-design-process","text":"Here we describe in generic terms, each step in the process of deriving event-linked microservice specifications. In following section we will describe in more detail how each of these steps plays out in the specific context of the container shipment example.","title":"Steps in the design process"},{"location":"design/readme/#list-of-generic-steps","text":"","title":"List of generic steps:"},{"location":"design/readme/#step-1-limit-the-context-and-scope-for-this-particular-build-sprint","text":"We assume that we are developing a particular build for a sprint within some agile development approach, deferring additional functions and complexity to later sprints: Working from the initial list of aggregates, select which aggregates will be included in this build For each aggregate possible choices are: (1) to completely skip and workaround the aggregate in this build (2) to include a full lifecycle implementation of the aggregate (3) to provide a simplified lifecycle implementation - typically a table of entities is initialized at start up, and state changes to existing entities are tracked Determine whether there are simulation services or predictive analytics service to be included in the build Identify the external query APIs and command APIs which this build should support Create entity lifecycle diagrams for entites having a full lifecycle implementation in this build / sprint.","title":"Step 1: Limit the context and scope for this particular build / sprint"},{"location":"design/readme/#step-2-identify-specific-microservices-in-each-aggregate","text":"Each aggregate will be implemented as some composition of: (1) a command microservice managing state changes to the entities in this aggregate (2) possibly one or more separate (CQRS) query services providing internal or external API query capabilities (3) additional simulation, predictive analytics or User Interface microservices The command microservice will be built around and manage a collection of active entites for the aggregate, keyed by some primary key The separation of each aggregate into specific component microservices as outlined above, will be a complete list of microservices for the build / sprint. Identify the data collections, and collection organization (keying structure) in each command and query microservice for this build.","title":"Step 2: Identify specific microservices in each aggregate"},{"location":"design/readme/#step-3-generate-microservice-interaction-diagrams-for-the-build","text":"The diagram will show API calls initiating state change. It shows for each interaction whether this is a synchronous API calls or an asynchronous event interaction via the event backbone. The diagram labels each specific event interaction between microservices trigerring a state change. Typically queries are synchronous API calls since the caller cannot usefully proceeed until a result is returned. From this we can extract: (1) a complete list of event types on each topic, with information passed on each event type. (2) the complete list of \u201clogic segments\u201d for each microservice processing action in response to an API call or initiating event. When, at the next level of detail, the individual fields in each event are specified and typed, the CloudEvents standard should be assumed as a starting point.","title":"Step 3: Generate microservice interaction diagrams for the build"},{"location":"design/readme/#step-4-specify-recovery-approach-in-case-a-microservice-fails","text":"If a microservice fails it will need to recover its internal state by resubscribing to one or more topics on the event backbone. In general, command and query microservices will have a standard pattern for doing this. Any custom event filtering and service specific logic should be specified.","title":"Step 4: Specify recovery approach in case a microservice fails"},{"location":"design/readme/#concepts-and-rationale-underlying-the-design-approach","text":"What is the difference between event information stored in the event backbone and state data stored in the microservices? The event information stored persistently in the event backbone is organized by topic and, within each topic, entirely by event time-of-occurrence. While the state information in a microservice is a list (collection) of all currently active entities of the owning aggregate (e.g. all orders, all voyages etc) and the current state of each such entity. The entity records are keyed by primary key, like an OrderID. While implementing microservice using event sourcing, CQRS, the persisted entity records are complementary to the historically organized information in the event backbone. When is it acceptable to be using synchronous interactions between services instead of asynchronous event interacts through the event backbone? For non-state-changing queries, for which the response is always instantaneously available a synchronous query call may be acceptable and will provide a simpler more understandable interface. Any processing which can be though of as being triggered by some state change in another aggregate should be modelled with an asynchronous event, because as the solution evolves other new microservices may also need to be aware of such event. We do not want to have to go back and change logic existing service where this event originated to have that microservice actively report the event to all potential consumers. How do we save microservices from having to maintain data collections with complex secondary indexing for which eventual consistency will be hard to implement? Each command microservice should do all its state changing updates using the primary key lookup only for its entities. Each asynchronous event interaction between microservices should carry primary entityIds ( orderID, VoyageID, shipID) for any entities associated with the interaction. Each query which might require speciaoized secondary indexing to respond to queries can be implemented in a separate CQRS query service which subscribes to events to do all internal updating and receives events from the event backbone in a ( Consistent) eventually correct order. This allows for recovery of any failed service by rebuilding it in \"eventually correct\" order.","title":"Concepts and rationale underlying the design approach"},{"location":"design/readme/#specific-application-to-container-shipment-example","text":"In this section we discuss how the generic steps introduced in previous section can be applied for the Container shipping example:","title":"Specific application to Container Shipment example"},{"location":"design/readme/#step1-context-and-scope-for-demonstration-build","text":"An initial scoping decision is that the demonstration will address shipment orders and shipment progress initiated by the \"manufacturer\" of the fresh goods with the shipment company. In the context of the example there is also discussion of manufacturer and retailer reaching some agreement on the goods to be delivered but this is not part of the demonstrated capabilities. The Event Storming analysis of the shipment problem was end-to-end and involved many aggregates including: Orders, Voyages, Trucking operations both at the source (manufacturer pickup) and at the destination (retailer delivery), Customs and export interactions, Container loading into ship at source port and unloading from ship at destination port, containers and fleet of ships. To have a simple initial demonstration build showing the role of event-driven architecture and event coupled microservices, as an initial step towards development of a more complete system using agile incremental development and deployment, the initial demonstration build makes the following simplifications and scoping decisions: This build will have no implementation of: Trucking operations, Customs and export, or Dockside Handling aggregates It will show a full lifecyle for a manufacturer user to place an order for shipment, seeing a filled container placed on board ship transported to the destination port and delivered. It will include a simulation service for ship movements - tracking the movement of ships carrying containers It will include simulation Container metrics such as temperature, current consumption while onboard ship It will provide a query for a user to track an order and the current location and state of the associated shipment It will include a real time analytic solution to assess problems happening within containers Based on the scope selection above, active aggregates in the build will be: Orders - with support for a complete order lifecycle Voyages - list of planned port to port passges with dates and manifests for each sip Containers - with allocation of a container to each order and temperature tracking of refrigerated containers Ships - with tracking of current voyage and current geographical position of each container ship The event backbone will be configured with a topic for each of the above aggregated. We expect to see multiple event types on each topic, but subscriptions and sequencing of events will be within these high level topics. Command APIs will be provided to: Place a new shipment order. Track an existing order, to confirm its booking state or to resolve the actual location and status of the container in transit. Modify an order request which could not be booked within the requested time window. A more complete and complex build could include an API for a shipping company person to optimally and manually assign orders to voyages, but for this initial demonstration we will automate this process and assign orders automatilly to the first voyage found meeting the requested requirements. Specifically, each order is assigned to the first located voyage: Going from the port nearest to pickup location To the port nearest the delivery location, Within the requested time window for pickup and delivery With available capacity for an additional container on that voyage. Additional APIs will be need: To initiate the overall demonstration To manage and view specific simulation component - container simulation and analytics and ship simulation and analytics.","title":"Step1: Context and scope for demonstration build"},{"location":"design/readme/#shipment-order-lifecycle-and-state-change-events","text":"The scoping decisions for the demonstration build listed above are reflected in a shipment order life cycle diagram shown below. A shipment order is initially created with an API call made by a manufacturer, or via a user interface (See demonstration script ). The order request specifies: The pickup location where empty container will be loaded The delivery location where the container is to be delivered to (we expect this to be in a remote country requiring a sea voyage) The shipment time window i.e.: Earliest date at which goods are available at pickup location for loading into the container Date by which delivery to the destination address is required Since our initial demonstration build expects to show refrigeration behavior and track preservation of a cold chain, we assume that orders are for some commodity which requires refrigeration during its shipment. A graphical view of this API with some additional field specification is provided in: When a new shipment order is placed, the shipping company must determine whether there is available capacity in some planned ship voyage which meets all the requirements specified by the manufacturer / customer. If there is a planned voyage with available capacity for additional container going from the source port nearest the pickup location to the destination port nearest to the delivery location then the order can transition to state=BOOKED and positive confirmation of the order returned to the requester. If no such voyage is available then the shipment order transitions to state=REJECTED (No Availability) and this is reported back to the requester. Once an order is BOOKED, then the expected dates and locations where for which a container will be needed are known. A request can be issued to book a specific (refrigerated) container for use with this shipment. We assume that the shipping company always has enough container available to meet expected shipment demand, hence the shipment order will transition to state=CONTAINER_ALLOCATED when this container booking is received. Since the scope for this demonstration build excluded the simulation of trucking operations to get the goods from the manufacturer's pickup location, export clearance and actual dockside loading operations, once an order has a container allocated it is \"ready to go\" and transitions to state=FULL_CONTAINER_VOYAGE_READY. The actual event of recording the container as being on board ship and at sea will not happen until simulated time in the demonstration reaches the scheduled start of the voyage on which that container is booked and the container ship assigned to that voyage is in the source port and also ready to go. At that point in simulated time, the state of the shipment order changes from state = FULL_CONTAINER_VOYAGE_READY to state = CONTAINER_ON_SHIP. While the order has state = CONTAINER_ON_SHIP, then we will be receiving temperature information from the Container simulation and Ship position information from the ship simulation service. Both provide a continuous streaming souces of information which should be considered part of the extended shipment state. After some period of simulated time, the ship will reach the destination port of the voyage. At this time the order transitions to state = CONTAINER_OFF_SHIP since our scope excluded simulation of actual dockside unloading information. Since we are not modelling customs clearance or trucking operations, there are no further events to be modeled until the order state = CONTAINER_DELIVERED. Since we are not modelling invoicing and billing operations the Container can be deallocated from this order and returned to some pool of free containers. When that has occurred the order state can be considered state = ORDER_COMPLETED. We have described the nornal, exception-free path first. There are two exception cases modelled: At the time a new shipment order is requested, there may be no voyage with available capacity meeting the location and time requirements of the request. When this occurs, the manufacturer/user is informed and the order state becomes state= REJECTED (No Availability). At this point, the user can modify the order with a second API requests changing dates or possibly locations. This retry request could still fail returning the order back to state = REJECTED ( No availability). Alternatively the changes in dates and location could be enough for an available voyage to be found. When this occurs the order will transition to state = BOOKED modified. If an API call to modify an order is made and the order is in some state different from state=REJECTED (No availability), we reject the API request. There could be race conditions, the order is in the process of being assigned to a voyage, or complex recovery issues. What if the order is already in a container and at sea when a modify order is received ? Full treatment of these complex business specific issues is out of scope and avoided by the state check in the modify order call API call We also model the exception condition when the refrigeration unit in a container fails or is misset or over loaded. If the temperature in the container goes outside the service level range for that shipment the goods must be considered spoiled. The order will transition from state = CONTAINER_ON_SHIP to state = ORDER_SPOILED (Temperature out of Range). Some complex business recovery such as compensating the customer and possibly scheduling a replacement shipment may be required. The details will be contract specific and outside the scope, but we do include the use of Streaming event container analytics to detect the spoilage and use rule based real-time /edge adjustments of the refrigeration gear to avoid spoilage in the demonstration simulation.","title":"Shipment order lifecycle and state change events"},{"location":"design/readme/#step-2-microservices-and-microservice-owned-data-for-demonstration-build","text":"In this step we fix the specific microservices for each aggregate and the data organization for each microservice.","title":"Step 2: microservices and microservice owned data for demonstration build"},{"location":"design/readme/#orders-aggregate","text":"For Orders we are implementing the CQRS pattern and we will need an orders-command-ms which will maintain a list of all current active orders and the current state of each order. The order state will as described above. The collection of active orders will be keyed by orderID. The orders-command-ms will offer APIs for create order and modify order since these are external interactions. It makes sense to use CQRS and separate out order tracking into a separate oders-query-ms since: The demand for order tracking might have significantly more intense scalability needs than order commands. Orders are typically created once and changes state a handful of times. There could be many different users querying status of a particular orders independently and each requesting tracking multiple times for each order to determine if there is some delay expected. Order state tracking information should probably be organized by requesting customer NOT by order ID: since customers should be allowed to see status on their own orders but not on other customer's orders when the shipping company is tracking an order it is most frequently doing so on behalf of a specific customer With this approach orders-query-ms becomes a CQRS query service with internal state updated from the event backbone, and an order tracking API.","title":"Orders Aggregate"},{"location":"design/readme/#voyages-aggregate","text":"For Voyages we will need a voyages-command-ms which will maintain a list of all voyages and their current state. In any given run of the demonstration we will work with a fixed set of voyages - effectively the schedule for the container fleet - so there is no need for an API to create additional voyages. The voyage definition will be read from file when the build is initializing. We expect this voyage data to be well formed: each voyage has a container ship in the fleet allocated to make the voyage the voyages assigned to any one ship are properly \"chained\". For the sequence of voyages assigned to any one container ship, the destination port of the nth voyage is always the start port of the (n+1)th voyage. The states of a voyage are: SCHEDULED - in this state it can accept order bookings, knows how much free space is available for additional bookings, and knows the orderIDs of each shipment already booked on the voyage IN_PROGRESS - in this state it includes a manifest a list of the orderIDs and containerIDs on board the ship COMPLETED - a voyage in the completed state supports tracing continers, may know which containers in the voyage were spoiled etc It will be helpful for the voyage-command-ms to include a query service to lookup voyages with a particular source port and destination port in a particular time windaw. This will help process booking request event but does not need to be an external API hence there is no strong argument for realizing this as a separate CQRS query service.","title":"Voyages Aggregate"},{"location":"design/readme/#containers-aggregate","text":"For Containers we will use a containers-command-ms to maintain a list of defined containerIDs and track the state of each container. A fixed set of valid container IDs will be initialized at demonstration start time. As noted previously we will assume this to be enough for all requested orders to be assigned a container without availability issues. Since the collection of containers is fixed the component will not need a command API The container current state maintained in container-command-ms is: state = FREE - this container is not in use and is available to be assigned to a new shipment order state = ALLOCATED - this container is allocated to an order orderID and potentially in use for that shipment. We will be modelling and performing streaming analytics on temperature inside a (refrigerated) container. Hence there will be a separate services performing this streaming analytics and simulation: container-streaming-svc. Conceptually, while a container is ALLOCATED to a shipment order with state = CONTAINER_ON_SHIP, its internal temperature and power usage will be maintained as streaming state by the container-streaming-svc.","title":"Containers Aggregate"},{"location":"design/readme/#fleetships-aggregate","text":"For Ships we will have a monolithic fleet simulation service providing continuous simulation of ship position for each ship and modelling of ship events. This service will include a UI to enable viewing the positions and states of the ships. It may have a separate UI to control the overall demonstration. There is no requirement for any separate microservice maintining additional information on ship state.","title":"Fleet/Ships Aggregate"},{"location":"design/readme/#step3-specifing-all-interactions-in-a-logic-flow-for-the-demonstration-build","text":"Using the understanding of the event flow from the Event Storming session, the scoping of this build, the list of microservices and data within each microservices developed in the steps above, we can write out in a complete interraction flow. This flow illustrates how the microservices are linked together via the Event backbone using event interractions for all non API interractions between distinct microservices.","title":"Step3: Specifing all interactions in a logic flow for the demonstration build"},{"location":"design/readme/#command-microservice-interactions-order-create-through-voyage-start-with-container-on-board","text":"The diagram below shows all command interactions from initial order creation through voyage start. The grey (shaded) columns of processing blocks are organized to show processing by the different command microservices. Column 1 shows processing by the orders-command-ms Column 2 shows processing by the voyages-command-ms Column 3 shows processing by the containers-command-ms and in a later figure by containers-streaming-ms Column 4 shows processing by the fleet/ships-simulator-ms Comments on steps in the command flow: A new shipment order request is initiated with the synchronous createOrder API at top left The orders-command-ms will create a new order record in its tale of active orders and populate it with order details. A NewOrder event is emitted on the Orders Topic. The generated orderID or the new shipment order is returned to the requester in the createOrder response. This enables the requester to query the status of an order and possibly modify the parameters of an unbooked order. The voyages-command-ms subscribes to all newOrder events on the Orders topic and tries to assign each new order to an available voyage: This operation is simplified by internally maintaining some list of vayages organized by port pair ( Starting port - ending port combination) and by time within port pair. Using such a list each voyage matching the port pair requirement of the new order can be checked or available capacity. If a voyage meeting all requirements for the new order is found, a booking event is emitted; if not, a rejected (No availability) event is emitted. A booked event causes state change in both the voyage - available capacity reduced, new order added to bookings - and to the order. We choose to make both booking and rejected (no Availabiity) events on the Orders topic rather than the Voyages topic. The orders-command-ms subscribes to Orders: booking and to Orders: Rejected (no availability) events and updates the current state of the affected order with the received information. For bookings, the current state of order is updated with the booking information including VoyageID and now specific pickup and delivery expected dates A rejected order has its state updated to rejected. This enables the requester to modify the order, suggesting different required dates or locations and trigerring a new search for a voyage meeting the modified requirements. Booked orders now have a specific schedule from the booked voyage of when they will need a container allocated for their use A Containers: needEmpty event is emitted to get a specific container allocated for use by the booked shipment The containers-command-ms subscribes to Containers: needEmpty events and allocates an available container for each one: This microservice is maintaining a list of all containers and their current states. For each incoming needEmpty event, it assigns a free container to that order and emits an Orders: allocatedContainer event specifying the containerID of the allocated container. It is very natural/necessary for the allocation of a container to be reported as an asynchronous event since this may occur at any time before the container is needed, possibly significanly later that he Containers:needEmpty event occurs We make Orders: allocatedContainer an event on the Orders topic since that is the most significant state change which it drives. The orders-command-ms subscribes to all Orders: allocatedContainer events and updates the order current state with its allocated containerID Once an order is booked on a voyage and has a container allocated for it to use, the actual physical process of shipment canbegin at this point. Since the delivery of empty container, loading it with goods at the pick up site, truck operations to get it to dockside etc are out of scope for this build, we can consider the container ready for its voyage at this point. Hence the Voyages:fullContainerReady event is emitted at this point by the orders-command-ms. This event includes the containerID of the allocated container. The voyages-command-ms subscribes to Voyages: fullContainerReady events and uses these to construct a complete manifest - the list of pairs which will travel on this voyage At this point the voyage-command-ms interacts with the fleet/ships-simulation-ms to simulate start of voyage We have shown this in the figure as a synchronous call to getNextVoyageInfo. This could also be handled with one or more event interactions The ship-simulator-ms will update the state of this ship to show the available containers and orders on board It will start the simulation of the ship moving on its course tocomplete the vogage The ship-simulator-ms willemit a Voyages: ShipStartedVoyage event The Voyages-command-ms receives this event and for each order/container in the manifest emits an Orders: ContainerOnShip event The orders-command-ms will subscribe to Orders: ContainerOnShip events and update the current state of each identified order with this information.","title":"Command microservice interactions - order create through voyage start with container on board"},{"location":"design/readme/#command-microservice-interaction-container-on-ship-at-sea-through-shipment-complete","text":"The diagram below shows all command interactions from container on ship in voyage through shipment delivered and order completed. As in the previous interaction diagram, the columns with grey/shaded processing blocks show work by (1) orders-command-ms (2) voyages-command-ms (3) containers-command-ms and containers-streaming-ms (4) fleet/ships-simulator service respectively. This diagram starts with containers on board a ship which is sailing on specific voyage and is at sea. The fleet/ships-simulator-ms repeatedly simulated movement of the ship along its course It emits Ships: GPSposition events recording the position of the ship at different points in simulated time. Similarly, while the ship is at sea, the container-streams-svc is continuously simulating temperature within the container and edge monitoring to adjust controls if necessary and to report a cold chain breach in that container if it occurs. This will result in a repeated stream of Containers: tempAndGpsState events reporting the temperature, GPS coordinates and possibly power consumption of the container There could also be on or more Containers: action events to adjust or reset controls of the refrigeration unit in the container These adjustment event are initiated by predictive real-ime analytics on the container state If the temperature in the container goes out of range and there is a cold chain failure, a Containers: temperature Out of Range event is emitted After some period of simulated time tracked by these ship position and container state repeated events, the ship will be simulated as arriving at the destination port of the voyage. The ship-simulator-ms emits a Voyages: ShipEndedVoyage event The voyages-command-ms subscribes to Voyages: ShipEndedVoyage and for each such event, emits Orders: containerOffShip It can do this because the current state record for each voyage includes the manifest of pairs which travelled on that voyage the current state of the voyage is updated to COMPLETED The orders-command-ms subscribes to Orders: containerOffShip and updates the state of all orders which have completed their shipping leg as a result of completion of their booked voyage Now, since simulation of the dockside unloading, customs processes, trucking operation to support deliver are out of scope for this build, we can consider the shipment delivered at this point orders-command-ms emits Orders: containerDelivered and marks this as current state of container With the shipment delivered, there is no further need for a container to be associated with this order; orders-command-ms emits Containers: containerReleased The containers-command-ms subscribes to Containers: containerReleased and marks the current state of the identified container as FREE and available to be allocated to other shipment orders The order-command-ms considers process of the shipment order complete at this point It emits Orders: orderComplete and marks this as the current state of the order A more complete and realistic build would statr invoicing and billing event at this poitn , but this was decided to be out of scope at this point The fleet/ships-simulator-ms will continue at this point to start the next voyage in its planned itenerary and interact with voyages-command-ms to do this this is a cycled repetition of start of voyage interaction discussed previously","title":"Command microservice interaction - container on ship at sea through shipment complete"},{"location":"design/readme/#query-microservice-service-cqrs-order-and-shipment-tracking-microservices","text":"The diagram below shows all interactions with the shipment tracking microservice. This microservice subscribes to many events carrying required information and supports one or more query APIs for different flavors of order and shipment tracking There could be multiple flavors of order and shipment tracking query APIs supported: Order confirmation query could address orders, bookings, rejections, modified orders etc Shipment state query could cover: container assignment, on board ship, ship position, off ship, delivery, etc * Cold chain certification query could want to augment the above with a full temperature log of the container while in transit and expect reporting on temperature range violations. Since we are using a CQRS approach, and all queries are non state changing, we could combine these multiple query levels into a single microservice or separate them out into separate microservices. If query load is intense there could be multiple instances of each such query microservices with load balancing of user requests. The design of these different flavors query services is essentially the same. The internal state data to respond to queries is obtained by subscribing to the necessary Topics. For cold chain and shipment reporting, this will involve all four topics Orders, Voyages, Containers and Ships. Internally the data will be organized by requester of the order, then by orderID, then current state and possibly summaries of repeated event history. The inteaction diagram 3 above illustrates this organization. For any order and shipment tracking query service there are synchronous APIs offered at one side and subscribed events received at the other to gather required state information from the vent backbone.","title":"Query microservice service  - CQRS Order and  Shipment tracking microservices"},{"location":"design/readme/#topics-event-types-and-the-event-emit-and-consumption-lists","text":"From the interaction diagrams we can compile a list of all event types which will occur in the build and check that they are organized into topics in a way which preserves all essential event sequencing. The diagram below lists the event types and topics, showing emitters ( publishers) and consumers ( subscribers) of each event type.","title":"Topics, event types and the event emit and consumption lists"},{"location":"design/readme/#step4-data-recovery-considerations-for-this-demonstration-build","text":"At this point the pattern for data recovery after a microservice failure is understood in principle, but specific recovery demonstration after a failure is out of scope at this point. Read more on EDA design pattern...","title":"Step4: Data recovery considerations for this demonstration build"}]}